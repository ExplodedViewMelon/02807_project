{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if you're starting over\n",
    "new_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if new_df:\n",
    "    # compute the best partition\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # compute modularity\n",
    "    mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "    pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"partition.pkl\", 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    with open(\"mod.pkl\", 'rb') as f:\n",
    "        mod = pickle.load(f)  \n",
    "        \n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_paths(G, root):\n",
    "    shortest_paths_dict = {root: [[root]]}\n",
    "    queue = deque([(root, [root])])\n",
    "\n",
    "    while queue:\n",
    "        s, path = queue.popleft()\n",
    "\n",
    "        for neighbor in G.neighbors(s):\n",
    "            new_path = path + [neighbor]\n",
    "            old_path = shortest_paths_dict.get(neighbor, [[None] * (len(new_path) + 1)])\n",
    "\n",
    "            if len(new_path) == len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor].append(new_path)\n",
    "            elif len(new_path) < len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor] = [new_path]\n",
    "                queue.append((neighbor, new_path))\n",
    "\n",
    "    return shortest_paths_dict\n",
    "\n",
    "def edge_betweenness_centrality(G):\n",
    "    edge_betweenness = defaultdict(float)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        shortest_paths_dict = bfs_shortest_paths(G, node)\n",
    "\n",
    "        for paths in shortest_paths_dict.values():\n",
    "            for path in paths:\n",
    "                for i in range(len(path) - 1):\n",
    "                    edge = (path[i], path[i + 1])\n",
    "                    edge_betweenness[edge] += 1.0\n",
    "\n",
    "    return edge_betweenness\n",
    "\n",
    "def girvan_newman_directed(G):\n",
    "    G_copy = G.copy()\n",
    "    communities = list(nx.weakly_connected_components(G_copy))\n",
    "    results = {0: communities}\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    while G_copy.number_of_edges() > 0:\n",
    "        edge_betweenness = edge_betweenness_centrality(G_copy)\n",
    "        max_betweenness = max(edge_betweenness.values())\n",
    "        highest_betweenness_edges = [edge for edge, value in edge_betweenness.items() if value == max_betweenness]\n",
    "        G_copy.remove_edges_from(highest_betweenness_edges)\n",
    "        components = list(nx.weakly_connected_components(G_copy))\n",
    "        results[step] = components\n",
    "        step += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def modularity(G, clusters_list):\n",
    "    Q = 0\n",
    "    m = len(list(G.edges()))\n",
    "    for aCommunity in clusters_list:\n",
    "        print(\"aCommunity\", aCommunity)\n",
    "        for v in list(aCommunity):\n",
    "            for w in list(aCommunity):\n",
    "                if v != w:\n",
    "                    avw = 1 if (v,w) in list(G.edges()) or (w,v) in list(G.edges()) else 0               \n",
    "                    new_term = avw - (G.degree(v)*G.degree(w))/(2*m)\n",
    "                    Q += new_term\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_modularity_for_all_communities(G, all_communities):\n",
    "    result = []\n",
    "    t = tqdm(total=len(list(all_communities.values())))\n",
    "    for aCommunityRepartition in list(all_communities.values()):\n",
    "        t.update()\n",
    "        aModularity = modularity(G, aCommunityRepartition)\n",
    "        result.append(\n",
    "            [aCommunityRepartition, aModularity]\n",
    "        )\n",
    "    t.close    \n",
    "    return result\n",
    "\n",
    "    \n",
    "B = nx.DiGraph()\n",
    "all_nodes = ['A','B','C','D','E','F','G','H']\n",
    "B.add_nodes_from(all_nodes)\n",
    "all_edges = [\n",
    "    ('E','D'),('E','F'),\n",
    "    ('F','G'),('D','G'),('D','B'),('B','A'),('B','C'),('A','H'),\n",
    "    ('D','F'),('A','C')\n",
    "]\n",
    "B.add_edges_from(all_edges)\n",
    "\n",
    "print('Finding communities...')\n",
    "all_com = girvan_newman_directed(B)\n",
    "\n",
    "print('Finding the modularity...')    \n",
    "all_clusters_with_modularity = compute_modularity_for_all_communities(B, all_com)\n",
    "\n",
    "print('Sorting')\n",
    "all_clusters_with_modularity.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Finding the best and pickling')\n",
    "best_cluster = all_clusters_with_modularity[0]\n",
    "print(best_cluster)\n",
    "#pickle.dump(best_cluster, open( \"best_cluster.pkl\", \"wb\" ) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dict where the key is the cluster number and the values\n",
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size_bigger_than_one = np.zeros(len(community_dict_bigger_than_one))\n",
    "\n",
    "for i,j in enumerate(community_dict_bigger_than_one):\n",
    "    community_size_bigger_than_one[i] = (len(community_dict_bigger_than_one[j])) \n",
    "\n",
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "print('There is:', sum(community_size == 1), 'communities with only 1 member')\n",
    "print('There is:', len(community_dict_bigger_than_one),'communities with more than 1 member')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)# Tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **paper** with their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenized words in each paper\n",
    "new_df = False\n",
    "if new_df:\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': \"For stereoscopic optical see-through head-mounted display...\n",
    "    abstracts = nx.get_node_attributes(G, 'abstract')\n",
    "    \n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': 'Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration'\n",
    "    titles = nx.get_node_attributes(G, 'title')\n",
    "\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': title + abstract}\n",
    "    paper_dict = {key: titles.get(key, '') + ' ' + abstracts.get(key, '') for key in set(abstracts) | set(titles)}\n",
    "    \n",
    "    # paper_dict_clean['345a2369-8198-46db-8ebb-b3f622b35381'] = ['scalable', 'feature', 'extraction', 'for', 'coarse',\n",
    "    paper_dict_clean = {key:  clean_and_tokenize(text) for key, text in paper_dict.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    pickle.dump(paper_dict_clean, open( \"paper_dict_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"paper_dict_clean.pkl\", 'rb') as f:\n",
    "        paper_dict_clean = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **cluster** with all words from the papers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making\n",
    "new_df = True\n",
    "\n",
    "if new_df:\n",
    "    community_texts_clean = {new_list: [] for new_list in range(len(community_dict_bigger_than_one))}\n",
    "    \n",
    "    for cluster_id, paper_ids in enumerate(community_dict_bigger_than_one.values()):\n",
    "        for paper_id in paper_ids:  \n",
    "            community_texts_clean[cluster_id].extend(paper_dict_clean[paper_id[0]])          \n",
    "    \n",
    "    #print('Pickling...')\n",
    "    #pickle.dump(community_text_clean, open( \"community_text_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"community_text_clean.pkl\", 'rb') as f:\n",
    "        community_text_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the dicts into nltk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into nltk format\n",
    "community_text_clean_text = { cluster_id: nltk.Text(text) for cluster_id, text in community_texts_clean.items() } \n",
    "\n",
    "# {'345a2369-8198-46db-8ebb-b3f622b35381': <Text: scalable feature extraction for coarse to fine jpeg...>,\n",
    "paper_dict_clean_text = { paper_id: nltk.Text(text) for paper_id, text in paper_dict_clean.items() } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dict_clean_text['345a2369-8198-46db-8ebb-b3f622b35381']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for each **community**, looking at the 'document' as a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "# TF_clusters[100] = Counter({'the': 28, 'of': 23,'in': 14,'we': 13,\n",
    "TF_clusters = {}\n",
    "word_set = set()\n",
    "\n",
    "for cluster, text in community_text_clean_text.items():\n",
    "    overall_freq = Counter()           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        word_set.update(set(list(fd.keys())))\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "    except:\n",
    "        print('Breaked')\n",
    "        continue\n",
    "        \n",
    "    TF_clusters[cluster] = overall_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TF_clusters, open( \"TF_clusters.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dict_clean_text['01f02fae-97df-4207-a386-a1bc8ec0853b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for each **paper** inside their community, looking at the 'document' as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each paper in the community\n",
    "TF_papers = {}\n",
    "word_set = set()\n",
    "\n",
    "for cluster, paper_ids in community_dict_bigger_than_one.items(): \n",
    "    TF_papers[cluster] = {}\n",
    "    \n",
    "    for paper_id in paper_ids:         \n",
    "        overall_freq = Counter()     \n",
    "                     \n",
    "        try:\n",
    "            text = paper_dict_clean_text[paper_id[0]]\n",
    "            print(text)\n",
    "            fd = nltk.FreqDist(text)\n",
    "            word_set.update(set(list(fd.keys())))\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "            \n",
    "        except:\n",
    "            print('Breaked')\n",
    "            continue\n",
    "        \n",
    "        TF_papers[cluster][paper_id[0]] = overall_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    occ = sum(1 for i in TF_clusters if word in TF_clusters[i])\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def tf_idf(number, com):    \n",
    "    total_words = len(TF_clusters[number])\n",
    "    return {word: (TF_clusters[number][word] / total_words) * idf(word) for word in com}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_all_communities = [tf_idf(i, TF_clusters.get(i)) for i in TF_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community = tf_idf_all_communities[6]\n",
    "sorted(community, key=community.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_idf_all_communities, open(\"tf_idf_all_communities.pkl\", \"wb\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
