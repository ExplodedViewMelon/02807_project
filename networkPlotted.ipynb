{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if you're starting over\n",
    "new_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if new_df:\n",
    "    # compute the best partition\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # compute modularity\n",
    "    mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "    pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"partition.pkl\", 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    with open(\"mod.pkl\", 'rb') as f:\n",
    "        mod = pickle.load(f)  \n",
    "        \n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_paths(G, root):\n",
    "    shortest_paths_dict = {root: [[root]]}\n",
    "    queue = deque([(root, [root])])\n",
    "\n",
    "    while queue:\n",
    "        s, path = queue.popleft()\n",
    "\n",
    "        for neighbor in G.neighbors(s):\n",
    "            new_path = path + [neighbor]\n",
    "            old_path = shortest_paths_dict.get(neighbor, [[None] * (len(new_path) + 1)])\n",
    "\n",
    "            if len(new_path) == len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor].append(new_path)\n",
    "            elif len(new_path) < len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor] = [new_path]\n",
    "                queue.append((neighbor, new_path))\n",
    "\n",
    "    return shortest_paths_dict\n",
    "\n",
    "def edge_betweenness_centrality(G):\n",
    "    edge_betweenness = defaultdict(float)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        shortest_paths_dict = bfs_shortest_paths(G, node)\n",
    "\n",
    "        for paths in shortest_paths_dict.values():\n",
    "            for path in paths:\n",
    "                for i in range(len(path) - 1):\n",
    "                    edge = (path[i], path[i + 1])\n",
    "                    edge_betweenness[edge] += 1.0\n",
    "\n",
    "    return edge_betweenness\n",
    "\n",
    "def girvan_newman_directed(G):\n",
    "    G_copy = G.copy()\n",
    "    communities = list(nx.weakly_connected_components(G_copy))\n",
    "    results = {0: communities}\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    while G_copy.number_of_edges() > 0:\n",
    "        edge_betweenness = edge_betweenness_centrality(G_copy)\n",
    "        max_betweenness = max(edge_betweenness.values())\n",
    "        highest_betweenness_edges = [edge for edge, value in edge_betweenness.items() if value == max_betweenness]\n",
    "        G_copy.remove_edges_from(highest_betweenness_edges)\n",
    "        components = list(nx.weakly_connected_components(G_copy))\n",
    "        results[step] = components\n",
    "        step += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def modularity(G, clusters_list):\n",
    "    Q = 0\n",
    "    m = len(list(G.edges()))\n",
    "    for aCommunity in clusters_list:\n",
    "        print(\"aCommunity\", aCommunity)\n",
    "        for v in list(aCommunity):\n",
    "            for w in list(aCommunity):\n",
    "                if v != w:\n",
    "                    avw = 1 if (v,w) in list(G.edges()) or (w,v) in list(G.edges()) else 0               \n",
    "                    new_term = avw - (G.degree(v)*G.degree(w))/(2*m)\n",
    "                    Q += new_term\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_modularity_for_all_communities(G, all_communities):\n",
    "    result = []\n",
    "    t = tqdm(total=len(list(all_communities.values())))\n",
    "    for aCommunityRepartition in list(all_communities.values()):\n",
    "        t.update()\n",
    "        aModularity = modularity(G, aCommunityRepartition)\n",
    "        result.append(\n",
    "            [aCommunityRepartition, aModularity]\n",
    "        )\n",
    "    t.close    \n",
    "    return result\n",
    "\n",
    "\n",
    "print('Finding communities...')\n",
    "all_com = girvan_newman_directed(G)\n",
    "\n",
    "print('Finding the modularity...')    \n",
    "all_clusters_with_modularity = compute_modularity_for_all_communities(G, all_com)\n",
    "\n",
    "print('Sorting')\n",
    "all_clusters_with_modularity.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Finding the best and pickling')\n",
    "best_cluster = all_clusters_with_modularity[0]\n",
    "print(best_cluster)\n",
    "#pickle.dump(best_cluster, open( \"best_cluster.pkl\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dict where the key is the cluster number and the values\n",
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is: 9085 communities with only 1 member\n",
      "There is: 1515 communities with more than 1 member\n"
     ]
    }
   ],
   "source": [
    "community_size_bigger_than_one = np.zeros(len(community_dict_bigger_than_one))\n",
    "\n",
    "for i,j in enumerate(community_dict_bigger_than_one):\n",
    "    community_size_bigger_than_one[i] = (len(community_dict_bigger_than_one[j])) \n",
    "\n",
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "print('There is:', sum(community_size == 1), 'communities with only 1 member')\n",
    "print('There is:', len(community_dict_bigger_than_one),'communities with more than 1 member')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)# Tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **paper** with their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenized words in each paper\n",
    "if new_df:\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': \"For stereoscopic optical see-through head-mounted display...\n",
    "    abstracts = nx.get_node_attributes(G, 'abstract')\n",
    "    \n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': 'Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration'\n",
    "    titles = nx.get_node_attributes(G, 'title')\n",
    "\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': title + abstract}\n",
    "    paper_dict = {key: titles.get(key, '') + ' ' + abstracts.get(key, '') for key in set(abstracts) | set(titles)}\n",
    "    \n",
    "    # paper_dict_clean['345a2369-8198-46db-8ebb-b3f622b35381'] = ['scalable', 'feature', 'extraction', 'for', 'coarse',\n",
    "    paper_dict_clean = {key:  clean_and_tokenize(text) for key, text in paper_dict.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    pickle.dump(paper_dict_clean, open( \"paper_dict_clean.pkl\", \"wb\" ) )\n",
    "    pickle.dump(abstracts, open( \"abstracts.pkl\", \"wb\" ) )\n",
    "    pickle.dump(titles, open( \"titles.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open(\"paper_dict_clean.pkl\", 'rb') as f:\n",
    "        paper_dict_clean = pickle.load(f)\n",
    "    \n",
    "    with open(\"abstracts.pkl\", 'rb') as f:\n",
    "        abstracts = pickle.load(f)\n",
    "    \n",
    "    with open(\"titles.pkl\", 'rb') as f:\n",
    "        titles = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **cluster** with all words from the papers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if new_df:\n",
    "    community_texts_clean = {new_list: [] for new_list in range(len(community_dict_bigger_than_one))}\n",
    "    \n",
    "    for cluster_id, paper_ids in enumerate(community_dict_bigger_than_one.values()):\n",
    "        for paper_id in paper_ids:  \n",
    "            community_texts_clean[cluster_id].extend(paper_dict_clean[paper_id[0]])          \n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(community_texts_clean, open( \"community_texts_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"community_texts_clean.pkl\", 'rb') as f:\n",
    "        community_texts_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the dicts into nltk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into nltk format\n",
    "community_text_clean_text = { cluster_id: nltk.Text(text) for cluster_id, text in community_texts_clean.items() } \n",
    "\n",
    "# {'345a2369-8198-46db-8ebb-b3f622b35381': <Text: scalable feature extraction for coarse to fine jpeg...>,\n",
    "paper_dict_clean_text = { paper_id: nltk.Text(text) for paper_id, text in paper_dict_clean.items() } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **community**, looking at the 'document' as a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "if new_df:\n",
    "    \n",
    "    # TF_clusters[100] = Counter({'the': 28, 'of': 23,'in': 14,'we': 13,\n",
    "    TF_clusters = {}\n",
    "\n",
    "    for cluster_, text in enumerate(community_text_clean_text.values()):\n",
    "        overall_freq = Counter()           \n",
    "        try:\n",
    "            fd = nltk.FreqDist(text)\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "        except:\n",
    "            print('Breaked')\n",
    "            continue\n",
    "            \n",
    "        TF_clusters[cluster_] = overall_freq\n",
    "\n",
    "    pickle.dump(TF_clusters, open( \"TF_clusters.pkl\", \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    with open(\"TF_clusters.pkl\", 'rb') as f:\n",
    "        TF_clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **paper** inside their community, looking at the 'document' as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_df:    \n",
    "    TF_papers = {}\n",
    "\n",
    "    for cluster_, paper_ids in enumerate(community_dict_bigger_than_one.values()): \n",
    "        TF_papers[cluster_] = {}\n",
    "        \n",
    "        for paper_id in paper_ids:         \n",
    "            overall_freq = Counter()     \n",
    "                    \n",
    "            try:\n",
    "                text = paper_dict_clean_text[paper_id[0]]\n",
    "                fd = nltk.FreqDist(text)\n",
    "                overall_freq = overall_freq + Counter(fd)\n",
    "                \n",
    "            except:\n",
    "                print('Breaked')\n",
    "                continue\n",
    "            \n",
    "            TF_papers[cluster_][paper_id[0]] = overall_freq\n",
    "            \n",
    "    pickle.dump(TF_papers, open( \"TF_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"TF_papers.pkl\", 'rb') as f:\n",
    "        TF_papers = pickle.load(f)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "new_df = True\n",
    "\n",
    "word_count = lambda word: sum(1 for i in TF_clusters if word in TF_clusters[i])\n",
    "\n",
    "def tf_idf(cluster_id, counter):  \n",
    "    # Total number of words in the cluster\n",
    "    total_words = counter.total()\n",
    "    \n",
    "    return {word: (TF_clusters[cluster_id][word] / total_words) * np.log( N / word_count(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "\n",
    "    # Number of clusters\n",
    "    N = len(TF_clusters)\n",
    "    \n",
    "    tf_idf_all_communities = [tf_idf(i, TF_clusters[i]) for i in TF_clusters]\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including top n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_items(dict_, n):\n",
    "    # Sort the dictionary by value in descending order and get the top n items\n",
    "    top_n_items = sorted(dict_.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Convert the list of tuples back to a dictionary\n",
    "    return dict(top_n_items)\n",
    "\n",
    "def tf_idf(cluster_id, counter):  \n",
    "    # Total number of words in the cluster\n",
    "    total_words = counter.total()\n",
    "    \n",
    "    return {word: (TF_clusters[cluster_id][word] / total_words) * np.log( N / word_count(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "\n",
    "    # Number of clusters\n",
    "    N = len(TF_clusters)\n",
    "    \n",
    "    tf_idf_all_communities = {i: tf_idf(i, TF_clusters[i]) for i in TF_clusters}\n",
    "    top_tf_idf_all_communities = {i: get_top_n_items(tf_idf_all_communities[i], 40) for i in tf_idf_all_communities}\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    pickle.dump(top_tf_idf_all_communities, open( \"top_tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)\n",
    "    with open(\"top_tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        top_tf_idf_all_communities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF for each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters:   0%|                                                                                                                                            | 1/1515 [00:11<5:00:00, 11.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1515/1515 [02:48<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if new_df:\n",
    "    \n",
    "    # Dict to store the word counts\n",
    "    word_count_papers = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i in TF_papers:\n",
    "        for paper in TF_papers[i].values():\n",
    "            for word in paper:\n",
    "                word_count_papers[i][word] += 1\n",
    "\n",
    "    tf_idf_all_papers = {}\n",
    "    \n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        # Number of papers inside the current cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "\n",
    "        def tf_idf_papers(counter): \n",
    "\n",
    "            # Total words in the current paper\n",
    "            total_words = counter.total()\n",
    "\n",
    "            return {word: (counter[word] / total_words) * np.log( N / word_count_papers[cluster_id][word] ) for word in counter}\n",
    "\n",
    "        tf_idf_all_papers[cluster_id] = {paper_id_: tf_idf_papers(counter_) for paper_id_, counter_ in papers.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    #pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including the top n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1515/1515 [03:03<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/zhome/5e/c/137246/02807_project/networkPlotted.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         top_tf_idf_all_papers[cluster_id] \u001b[39m=\u001b[39m {paper_id_: get_top_n_items(tf_idf_all_papers[cluster_id][paper_id_], \u001b[39m40\u001b[39m) \u001b[39mfor\u001b[39;00m paper_id_ \u001b[39min\u001b[39;00m tf_idf_all_papers[cluster_id]}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPickling...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(tf_idf_all_papers, \u001b[39mopen\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mtf_idf_all_papers.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m ) )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(top_tf_idf_all_papers, \u001b[39mopen\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mtop_tf_idf_all_papers.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m ) )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "def get_top_n_items(dict_, n):\n",
    "    # Sort the dictionary by value in descending order and get the top n items\n",
    "    top_n_items = sorted(dict_.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Convert the list of tuples back to a dictionary\n",
    "    return dict(top_n_items)\n",
    "\n",
    "def tf_idf_papers(counter): \n",
    "    # Total words in the current paper\n",
    "    total_words = counter.total()\n",
    "\n",
    "    return {word: (counter[word] / total_words) * np.log( N / word_count_papers[cluster_id][word] ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "    # Dict to store the word counts\n",
    "    word_count_papers = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i in TF_papers:\n",
    "        for paper in TF_papers[i].values():\n",
    "            for word in paper:\n",
    "                word_count_papers[i][word] += 1\n",
    "\n",
    "    tf_idf_all_papers = {}\n",
    "    top_tf_idf_all_papers = {}\n",
    "\n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        # Number of papers inside the current cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "\n",
    "        tf_idf_all_papers[cluster_id] = {paper_id_: tf_idf_papers(counter_) for paper_id_, counter_ in papers.items()}\n",
    "        top_tf_idf_all_papers[cluster_id] = {paper_id_: get_top_n_items(tf_idf_all_papers[cluster_id][paper_id_], 40) for paper_id_ in tf_idf_all_papers[cluster_id]}\n",
    "\n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    pickle.dump(top_tf_idf_all_papers, open( \"top_tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f)\n",
    "    with open(\"top_tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        top_tf_idf_all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stereoscopic', 'calibration', 'unreliable', 'eyes', 'optical']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community = tf_idf_all_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b']\n",
    "sorted(community, key=community.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for user-input. At cluster level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input:  *Assessing the Impact of Climate Change on Coral Reef Ecosystems*    This project aims to explore and synthesize the current body of research concerning the effects of climate change on coral reef ecosystems across the globe. The focus will be on understanding how rising sea temperatures and ocean acidification are impacting coral biodiversity, altering reef structures, and affecting the adaptive mechanisms of various coral species. Additionally, this project will delve into the long-term ecological implications of these changes on marine life and ecosystem health.\n",
      "TF-IDF scores:  {'assessing': 0.04134551328849299, 'the': 0.00033068826463505583, 'impact': 0.028962161782401373, 'of': 0.0003151499593436911, 'climate': 0.09040024269826924, 'change': 0.059401356681932575, 'on': 0.009270227250639671, 'coral': 0.2060684973518799, 'reef': 0.16728976386886465, 'ecosystems': 0.0941874973323094, 'this': 0.003262533652884385, 'project': 0.06593556729387924, 'aims': 0.03677457396840726, 'to': 0.00044838296929385454, 'explore': 0.035781935766751885, 'and': 0.00044208982428607896, 'synthesize': 0.04669015876525373, 'current': 0.023244436783521505, 'body': 0.0348657328960837, 'research': 0.02386965147737206, 'concerning': 0.03374445652632535, 'effects': 0.022236304792687712, 'across': 0.03443278475119233, 'globe': 0.04751150247343173, 'focus': 0.03100799817438541, 'will': 0.042802804257421084, 'be': 0.005113266230248024, 'understanding': 0.032005258687069445, 'how': 0.022650425222275995, 'rising': 0.0455555137675832, 'sea': 0.04520012134913462, 'temperatures': 0.03843840661573058, 'ocean': 0.045921843037425504, 'acidification': 0.06401500677247804, 'are': 0.0023080139168344827, 'impacting': 0.04839374023716651, 'biodiversity': 0.05093628905023865, 'altering': 0.045921843037425504, 'structures': 0.028182662623121527, 'affecting': 0.04451966404151618, 'adaptive': 0.036103705676131886, 'mechanisms': 0.03531502251492711, 'various': 0.021640942642989363, 'species': 0.03969269846879994, 'additionally': 0.0382416292115495, 'delve': 0.04985329169064666, 'into': 0.01646427934789331, 'long': 0.0263955386195582, 'term': 0.028872987118959026, 'ecological': 0.04520012134913462, 'implications': 0.03626791189199302, 'these': 0.010184190379037094, 'changes': 0.028522821818884103, 'marine': 0.04751150247343173, 'life': 0.03457535845412942, 'ecosystem': 0.0470937486661547, 'health': 0.039915223861801755}\n",
      "[('coral', 0.2060684973518799), ('reef', 0.16728976386886465), ('ecosystems', 0.0941874973323094), ('climate', 0.09040024269826924), ('project', 0.06593556729387924), ('acidification', 0.06401500677247804), ('change', 0.059401356681932575), ('biodiversity', 0.05093628905023865), ('delve', 0.04985329169064666), ('impacting', 0.04839374023716651)]\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print('The user input: ', text)\n",
    "\n",
    "def text_cluster(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf(word):\n",
    "    return max(1, sum(word in cluster for cluster in TF_clusters.values()))\n",
    "\n",
    "def text_tf_idf(tf_words):\n",
    "    tf_idf_scores = {}\n",
    "    N = len(TF_clusters) \n",
    "    for word in tf_words:\n",
    "        tf = TF_text[word]/TF_text.total()\n",
    "        idfreq = np.log( N / text_idf(word) )\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "\n",
    "print('TF-IDF scores: ', cLevel_tf_idf)\n",
    "\n",
    "cLevel_sorted = sorted(cLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "print(cLevel_sorted[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(tfidf, keywords):\n",
    "    keywords_40 = [x[0] for x in keywords]\n",
    "    set_input_words = set(tfidf.keys())\n",
    "    set_keywords = set(keywords_40)\n",
    "    \n",
    "    intersection = len(set_input_words & set_keywords)\n",
    "    union = len(set_input_words | set_keywords)\n",
    "    \n",
    "    similarity = intersection / union if union != 0 else 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['flow', 'binding', 'heparin', 'growth', 'hollow', 'capture', 'fgf', 'fibers', 'factor', 'endothelial', 'pulsatile', 'heparan', 'receptor', 'circulation', 'sulfate', 'fibroblast', 'equations', 'bioreactor', 'predicted', 'rates', 'dissociation', 'pdes', 'odes', 'experimental', 'proteins', 'fgfr', 'biologicals', 'fiber', 'delivery', 'synthetic', 'model', 'hspg', 'reactions', 'was', 'proteoglycan', 'proteoglycans', 'quantitative', 'under', 'transport', 'cells'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tf_idf_all_communities[100].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cluster choosen was: 585 \n",
      "With a jaccard similarity of: 0.05263157894736842\n"
     ]
    }
   ],
   "source": [
    "sim_cluster = {}\n",
    "for i in range(len(tf_idf_all_communities)):\n",
    "    sim_cluster[i] = jaccard_similarity(top_tf_idf_all_communities[i], cLevel_sorted)\n",
    "    \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_cluster, best_cluster_similarity = sorted_similarity[0]\n",
    "print('The cluster choosen was:', best_cluster, '\\nWith a jaccard similarity of:', best_cluster_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words from the located cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x[0] for x in tf_idf_all_communities[best_cluster]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF inside the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores for inside cluster:  585 , is  [('coral', 0.03300700859809263), ('reef', 0.024755256448569473), ('ecosystems', 0.016503504299046314), ('project', 0.016503504299046314), ('will', 0.016503504299046314), ('assessing', 0.008251752149523157), ('aims', 0.008251752149523157), ('explore', 0.008251752149523157), ('synthesize', 0.008251752149523157), ('current', 0.008251752149523157), ('body', 0.008251752149523157), ('concerning', 0.008251752149523157), ('effects', 0.008251752149523157), ('across', 0.008251752149523157), ('globe', 0.008251752149523157), ('focus', 0.008251752149523157), ('be', 0.008251752149523157), ('understanding', 0.008251752149523157), ('how', 0.008251752149523157), ('rising', 0.008251752149523157), ('sea', 0.008251752149523157), ('temperatures', 0.008251752149523157), ('ocean', 0.008251752149523157), ('acidification', 0.008251752149523157), ('impacting', 0.008251752149523157), ('biodiversity', 0.008251752149523157), ('altering', 0.008251752149523157), ('structures', 0.008251752149523157), ('affecting', 0.008251752149523157), ('adaptive', 0.008251752149523157), ('mechanisms', 0.008251752149523157), ('various', 0.008251752149523157), ('species', 0.008251752149523157), ('additionally', 0.008251752149523157), ('delve', 0.008251752149523157), ('into', 0.008251752149523157), ('long', 0.008251752149523157), ('term', 0.008251752149523157), ('ecological', 0.008251752149523157), ('implications', 0.008251752149523157)]\n"
     ]
    }
   ],
   "source": [
    "def text_paper(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf_paper(word):\n",
    "    return max(1,sum(word in cluster for cluster in TF_papers[best_cluster].values()))\n",
    "\n",
    "def text_tf_idf_paper(tf_words, best_cluster):\n",
    "    N = len(TF_papers[best_cluster])\n",
    "    tf_idf_scores = {} \n",
    "    for word in tf_words:\n",
    "        tf = TF_paper[word]/TF_paper.total()\n",
    "        idfreq = np.log(N / text_idf_paper(word))\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "TF_paper = text_paper(text)\n",
    "dLevel_tf_idf = text_tf_idf_paper(TF_paper, best_cluster)\n",
    "dLevel_sorted = sorted(dLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "print('TF-IDF scores for inside cluster: ', str(best_cluster), ', is ', dLevel_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73d121f9-0014-434c-b689-e1024ebb95f8\n"
     ]
    }
   ],
   "source": [
    "for i in tf_idf_all_papers[best_cluster]:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_tf_idf_all_papers[best_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###   Papers with highest jaccard similarity   ###\n",
      "\n",
      "( Paper id, jaccard similarity score) \n",
      "('73d121f9-0014-434c-b689-e1024ebb95f8', 0.03896103896103896)\n",
      "('f188c8be-987b-4fce-984a-95a5105e5a8e', 0.012658227848101266)\n"
     ]
    }
   ],
   "source": [
    "sim_paper = {}\n",
    "for i in TF_papers[best_cluster]:\n",
    "    sim_paper[i] = jaccard_similarity(top_tf_idf_all_papers[best_cluster][i], dLevel_sorted)\n",
    "  \n",
    "sorted_similarity_paper = sorted(sim_paper.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "best_papers = sorted_similarity_paper[:10]\n",
    "print('\\n###   Papers with highest jaccard similarity   ###\\n')\n",
    "print('( Paper id, jaccard similarity score) ')\n",
    "for paper in best_papers:\n",
    "    print(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:  Systematically linking qualitative elements of scenarios across levels, scales, and sectors\n",
      "Abstract:  New scenarios for climate change research connect climate model results based on Representative Concentration Pathways to nested interpretations of Shared Socioeconomic Pathways. Socioeconomic drivers of emissions and determinants of impacts are now decoupled from climate model outputs. To retain scenario credibility, more internally consistent linking across scales must be achieved. This paper addresses this need, demonstrating a modification to cross impact balances (CIB), a method for systematically deriving qualitative socioeconomic scenarios. Traditionally CIB is performed with one cross-impact matrix. This poses limitations, as more than a few dozen scenario elements with sufficiently varied outcomes can become computationally infeasible to comprehensively explore. Through this paper, we introduce the concept of 'linked CIB', which takes the structure of judgements for how scenario elements interact to partition a single cross-impact matrix into multiple smaller matrices. Potentially, this enables analysis of large CIB matrices and ensures internally consistent linking of scenario elements across scales. Elements of multi-level qualitative scenarios have asymmetric interrelationships.Structures of interrelationships can be documented and partitioned into subsets.Analyses of subsets can systematically reconstitute a multi-level parent scenario.Internal consistency of multi-scale scenarios may be preserved with partitioning.\n",
      "\n",
      "Title:  Objectivity and a comparison of methodological scenario approaches for climate change research\n",
      "Abstract:  Climate change assessments rely upon scenarios of socioeconomic developments to conceptualize alternative outcomes for global greenhouse gas emissions. These are used in conjunction with climate models to make projections of future climate. Specifically, the estimations of greenhouse gas emissions based on socioeconomic scenarios constrain climate models in their outcomes of temperatures, precipitation, etc. Traditionally, the fundamental logic of the socioeconomic scenarios—that is, the logic that makes them plausible—is developed and prioritized using methods that are very subjective. This introduces a fundamental challenge for climate change assessment: The veracity of projections of future climate currently rests on subjective ground. We elaborate on these subjective aspects of scenarios in climate change research. We then consider an alternative method for developing scenarios, a systems dynamics approach called ‘Cross-Impact Balance’ (CIB) analysis. We discuss notions of ‘objective’ and ‘objectivity’ as criteria for distinguishing appropriate scenario methods for climate change research. We distinguish seven distinct meanings of ‘objective,’ and demonstrate that CIB analysis is more objective than traditional subjective approaches. However, we also consider criticisms concerning which of the seven meanings of ‘objective’ are appropriate for scenario work. Finally, we arrive at conclusions regarding which meanings of ‘objective’ and ‘objectivity’ are relevant for climate change research. Because scientific assessments uncover knowledge relevant to the responses of a real, independently existing climate system, this requires scenario methodologies employed in such studies to also uphold the seven meanings of ‘objective’ and ‘objectivity.’\n"
     ]
    }
   ],
   "source": [
    "for b in best_papers:\n",
    "    print('\\nTitle: ',titles[b[0]])\n",
    "    print('Abstract: ', abstracts[b[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged user input box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input:\n",
      " *Assessing the Impact of Climate Change on Coral Reef Ecosystems*    This project aims to explore and synthesize the current body of research concerning the effects of climate change on coral reef ecosystems across the globe. The focus will be on understanding how rising sea temperatures and ocean acidification are impacting coral biodiversity, altering reef structures, and affecting the adaptive mechanisms of various coral species. Additionally, this project will delve into the long-term ecological implications of these changes on marine life and ecosystem health.\n",
      "10 best TF-IDF scores:\n",
      "[('coral', 0.2060684973518799), ('reef', 0.16728976386886465), ('ecosystems', 0.0941874973323094), ('climate', 0.09040024269826924), ('project', 0.06593556729387924), ('acidification', 0.06401500677247804), ('change', 0.059401356681932575), ('biodiversity', 0.05093628905023865), ('delve', 0.04985329169064666), ('impacting', 0.04839374023716651)]\n",
      "## Best matching clusters: ##\n",
      "585 616 334\n",
      "{0: {0.012658227848101266}, 1: {0.012658227848101266}, 2: {0.02564102564102564}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/zhome/5e/c/137246/02807_project/networkPlotted.ipynb Cell 50\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y224sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m sorted_similarity_papers \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y224sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, sim_paper \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sim_papers):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y224sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     sorted_similarity_papers[i] \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(sim_papers[i]\u001b[39m.\u001b[39;49mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x:x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:\u001b[39m10\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y224sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, cluster \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(clusters):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#Y224sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m###   Cluster\u001b[39m\u001b[39m'\u001b[39m, i, \u001b[39m'\u001b[39m\u001b[39m- Papers with highest jaccard similarity   ###\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print('The user input:\\n', text)\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "cLevel_sorted = sorted(cLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "print('10 best TF-IDF scores:')\n",
    "print(cLevel_sorted[:10])\n",
    "\n",
    "# Similarity on cluster-level\n",
    "sim_cluster = {}\n",
    "for i in range(len(tf_idf_all_communities)):\n",
    "    sim_cluster[i] = jaccard_similarity(top_tf_idf_all_communities[i], cLevel_sorted)\n",
    "    \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "    \n",
    "c1, c2, c3 = sorted_similarity[0][0], sorted_similarity[1][0], sorted_similarity[2][0]\n",
    "\n",
    "print('## Best matching clusters: ##')\n",
    "print(c1, c2, c3)\n",
    "\n",
    "# Similarity on paper-level\n",
    "TF_paper = text_paper(text)\n",
    "dLevel_sorted = sorted(dLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "\n",
    "c1dLevel_tf_idf = text_tf_idf_paper(TF_paper, c1)\n",
    "c1dLevel_sorted = sorted(c1dLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "\n",
    "c2dLevel_tf_idf = text_tf_idf_paper(TF_paper,c2)\n",
    "c2dLevel_sorted = sorted(c2dLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "\n",
    "c3dLevel_tf_idf = text_tf_idf_paper(TF_paper, c3)\n",
    "c3dLevel_sorted = sorted(c3dLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "\n",
    "\n",
    "#for i in TF_papers[best_cluster]:\n",
    "#    sim_paper[i] = jaccard_similarity(top_tf_idf_all_papers[best_cluster][i], dLevel_sorted)\n",
    " \n",
    "clusters = [c1,c2,c3]\n",
    "sim_papers = {}\n",
    " \n",
    "for i, cluster in enumerate(clusters):\n",
    "    sim_papers[i] = {}\n",
    "    for n in TF_papers[cluster]:\n",
    "        sim_papers[i] = {jaccard_similarity(top_tf_idf_all_papers[cluster][n], dLevel_sorted)}\n",
    "\n",
    "print(sim_papers)\n",
    "sorted_similarity_papers = {}\n",
    "for i, sim_paper in enumerate(sim_papers):\n",
    "    sorted_similarity_papers[i] = sorted(sim_papers[i].items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "    \n",
    "for i, cluster in enumerate(clusters):\n",
    "    print('\\n###   Cluster', i, '- Papers with highest jaccard similarity   ###\\n')\n",
    "    print('( Paper id, jaccard similarity score) ')\n",
    "    for paper in sorted_similarity_papers[i]:\n",
    "        print(paper)\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print('\\n###   Papers recommended   ###')\n",
    "    for b in sorted_similarity_papers[i]:\n",
    "        print('\\nTitle: ',titles[b[0]])\n",
    "        print('Abstract: ', abstracts[b[0]])\n",
    "#c1sorted_similarity_paper = sorted(c1sim_paper.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "#c2sorted_similarity_paper = sorted(c2sim_paper.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "#c3sorted_similarity_paper = sorted(c3sim_paper.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "\n",
    "'''\n",
    "c1sim_paper = {}\n",
    "for i in TF_papers[c1]:\n",
    "    c1sim_paper[i] = jaccard_similarity(top_tf_idf_all_papers[c1][i], dLevel_tf_idf)\n",
    "    \n",
    "c2sim_paper = {}\n",
    "for i in TF_papers[c2]:\n",
    "    c2sim_paper[i] = jaccard_similarity(top_tf_idf_all_papers[c2][i], dLevel_tf_idf)\n",
    "    \n",
    "c3sim_paper = {}\n",
    "for i in TF_papers[c3]:\n",
    "    c3sim_paper[i] = jaccard_similarity(top_tf_idf_all_papers[c3][i], dLevel_tf_idf)\n",
    "\n",
    "\n",
    "c1best_papers = c1sorted_similarity_paper[:5]\n",
    "c2best_papers = c2sorted_similarity_paper[:5]\n",
    "c3best_papers = c3sorted_similarity_paper[:5]\n",
    "\n",
    "print('\\n###   Cluster 1 - Papers with highest jaccard similarity   ###\\n')\n",
    "print('( Paper id, jaccard similarity score) ')\n",
    "for paper in c1best_papers:\n",
    "    print(paper)\n",
    "    \n",
    "print('\\n###   Cluster 2 - Papers with highest jaccard similarity   ###\\n')\n",
    "print('( Paper id, jaccard similarity score) ')\n",
    "for paper in c2best_papers:\n",
    "    print(paper)\n",
    "    \n",
    "print('\\n###   Cluster 3 - Papers with highest jaccard similarity   ###\\n')\n",
    "print('( Paper id, jaccard similarity score) ')\n",
    "for paper in c3best_papers:\n",
    "    print(paper)\n",
    "\n",
    "print('\\n###   Papers recommended   ###')\n",
    "for b in c1best_papers:\n",
    "    print('\\nTitle: ',titles[b[0]])\n",
    "    print('Abstract: ', abstracts[b[0]])\n",
    "\n",
    "print('\\n###   Papers recommended   ###')\n",
    "for b in c2best_papers:\n",
    "    print('\\nTitle: ',titles[b[0]])\n",
    "    print('Abstract: ', abstracts[b[0]])\n",
    "    \n",
    "print('\\n###   Papers recommended   ###')\n",
    "for b in c3best_papers:\n",
    "    print('\\nTitle: ',titles[b[0]])\n",
    "    print('Abstract: ', abstracts[b[0]])\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, paper_:Counter):\n",
    "    return paper_.get(term)/paper_.total()\n",
    "\n",
    "def inv_doc_freq(term_, cluster_:dict):\n",
    "    N = len(cluster_)\n",
    "    docs =0\n",
    "    for _, p_counter in cluster_.items():\n",
    "        if term_ in p_counter:\n",
    "            docs +=1\n",
    "    docs_w_term = tuple((p_id,p_counter) for  p_id, p_counter in cluster_.items() if term_ in p_counter.keys())\n",
    "    return np.log(N/len(docs_w_term))\n",
    "\n",
    "tf_idf = lambda term, paper, cluster: term_freq(term, paper) * inv_doc_freq(term, cluster)\n",
    "\n",
    "def paper_tfidf(paper_:Counter, cluster_: dict):\n",
    "    terms_tfidf = Counter()\n",
    "    for term in paper_:\n",
    "        terms_tfidf[term] = tf_idf(term, paper_, cluster_)\n",
    "    return terms_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01f02fae-97df-4207-a386-a1bc8ec0853b\n",
      "076bca9b-b30c-4c2c-8148-3b9b5d9aa939\n"
     ]
    }
   ],
   "source": [
    "cluster0_keys = TF_papers[0].keys()\n",
    "\n",
    "print(list(cluster0_keys)[0])\n",
    "print(list(cluster0_keys)[1])\n",
    "print(list(cluster0_keys)[2])\n",
    "\n",
    "cluster_dummy = {\n",
    "    '01f02fae-97df-4207-a386-a1bc8ec0853b':TF_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b'],\n",
    "    '076bca9b-b30c-4c2c-8148-3b9b5d9aa939':TF_papers[0]['076bca9b-b30c-4c2c-8148-3b9b5d9aa939']\n",
    "}\n",
    "\n",
    "cluster = TF_papers[0]\n",
    "TFIDFs = dict.fromkeys(cluster.keys())\n",
    "for p_id, p_counter in cluster.items():\n",
    "    TFIDFs[p_id] = paper_tfidf(p_counter, cluster)\n",
    "    \n",
    "c0_df = DataFrame(columns=list(TF_clusters[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_papers(word,papers):\n",
    "    occ = sum(1 for paper in papers.values() if word in paper)\n",
    "    return np.log(10 / occ)\n",
    "\n",
    "def tf_idf_papers(cluster, paper_id, counter): \n",
    "    print(paper_id)\n",
    "    total_words = len(counter)\n",
    "    return {word: (counter[word] / total_words) * idf_papers(word, TF_papers[cluster]) for word in counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = True\n",
    "\n",
    "word_count_papers = lambda word: sum(1 for i in papers if word in papers[i])\n",
    "\n",
    "def tf_idf_papers(paper_id, counter): \n",
    "    print(paper_id)\n",
    "    \n",
    "    # Total words in the current paper\n",
    "    total_words = counter.total()\n",
    "       \n",
    "    return {word: (counter[word] / total_words) * np.log( N / word_count_papers(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "    tf_idf_all_papers = {}\n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        \n",
    "        # Number of papers inside the currect cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "        \n",
    "        tf_idf_all_papers[cluster_id] = [tf_idf_papers(paper_id_, counter_) for paper_id_, counter_ in papers.items()]\n",
    "    \n",
    "    pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "def tf_idf(cluster_id, counter):  \n",
    "    # Total number of words in the cluster\n",
    "    total_words = counter.total()\n",
    "    \n",
    "    # Calculate TF-IDF for each word and sort the results\n",
    "    tf_idf_values = {word: (TF_clusters[cluster_id][word] / total_words) * np.log( N / word_count(word) ) for word in counter}\n",
    "    sorted_tf_idf_values = sorted(tf_idf_values.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top 40 items\n",
    "    return sorted_tf_idf_values[:40]\n",
    "\n",
    "if new_df:\n",
    "\n",
    "    # Number of clusters\n",
    "    N = len(TF_clusters)\n",
    "    \n",
    "    tf_idf_all_communities = {i: tf_idf(i, TF_clusters[i]) for i in TF_clusters}\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
