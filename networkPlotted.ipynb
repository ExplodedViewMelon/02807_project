{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if you're starting over\n",
    "new_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if new_df:\n",
    "    # compute the best partition\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # compute modularity\n",
    "    mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "    pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"partition.pkl\", 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    with open(\"mod.pkl\", 'rb') as f:\n",
    "        mod = pickle.load(f)  \n",
    "        \n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_paths(G, root):\n",
    "    shortest_paths_dict = {root: [[root]]}\n",
    "    queue = deque([(root, [root])])\n",
    "\n",
    "    while queue:\n",
    "        s, path = queue.popleft()\n",
    "\n",
    "        for neighbor in G.neighbors(s):\n",
    "            new_path = path + [neighbor]\n",
    "            old_path = shortest_paths_dict.get(neighbor, [[None] * (len(new_path) + 1)])\n",
    "\n",
    "            if len(new_path) == len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor].append(new_path)\n",
    "            elif len(new_path) < len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor] = [new_path]\n",
    "                queue.append((neighbor, new_path))\n",
    "\n",
    "    return shortest_paths_dict\n",
    "\n",
    "def edge_betweenness_centrality(G):\n",
    "    edge_betweenness = defaultdict(float)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        shortest_paths_dict = bfs_shortest_paths(G, node)\n",
    "\n",
    "        for paths in shortest_paths_dict.values():\n",
    "            for path in paths:\n",
    "                for i in range(len(path) - 1):\n",
    "                    edge = (path[i], path[i + 1])\n",
    "                    edge_betweenness[edge] += 1.0\n",
    "\n",
    "    return edge_betweenness\n",
    "\n",
    "def girvan_newman_directed(G):\n",
    "    G_copy = G.copy()\n",
    "    communities = list(nx.weakly_connected_components(G_copy))\n",
    "    results = {0: communities}\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    while G_copy.number_of_edges() > 0:\n",
    "        edge_betweenness = edge_betweenness_centrality(G_copy)\n",
    "        max_betweenness = max(edge_betweenness.values())\n",
    "        highest_betweenness_edges = [edge for edge, value in edge_betweenness.items() if value == max_betweenness]\n",
    "        G_copy.remove_edges_from(highest_betweenness_edges)\n",
    "        components = list(nx.weakly_connected_components(G_copy))\n",
    "        results[step] = components\n",
    "        step += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def modularity(G, clusters_list):\n",
    "    Q = 0\n",
    "    m = len(list(G.edges()))\n",
    "    for aCommunity in clusters_list:\n",
    "        print(\"aCommunity\", aCommunity)\n",
    "        for v in list(aCommunity):\n",
    "            for w in list(aCommunity):\n",
    "                if v != w:\n",
    "                    avw = 1 if (v,w) in list(G.edges()) or (w,v) in list(G.edges()) else 0               \n",
    "                    new_term = avw - (G.degree(v)*G.degree(w))/(2*m)\n",
    "                    Q += new_term\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_modularity_for_all_communities(G, all_communities):\n",
    "    result = []\n",
    "    t = tqdm(total=len(list(all_communities.values())))\n",
    "    for aCommunityRepartition in list(all_communities.values()):\n",
    "        t.update()\n",
    "        aModularity = modularity(G, aCommunityRepartition)\n",
    "        result.append(\n",
    "            [aCommunityRepartition, aModularity]\n",
    "        )\n",
    "    t.close    \n",
    "    return result\n",
    "\n",
    "    \n",
    "B = nx.DiGraph()\n",
    "all_nodes = ['A','B','C','D','E','F','G','H']\n",
    "B.add_nodes_from(all_nodes)\n",
    "all_edges = [\n",
    "    ('E','D'),('E','F'),\n",
    "    ('F','G'),('D','G'),('D','B'),('B','A'),('B','C'),('A','H'),\n",
    "    ('D','F'),('A','C')\n",
    "]\n",
    "B.add_edges_from(all_edges)\n",
    "\n",
    "print('Finding communities...')\n",
    "all_com = girvan_newman_directed(B)\n",
    "\n",
    "print('Finding the modularity...')    \n",
    "all_clusters_with_modularity = compute_modularity_for_all_communities(B, all_com)\n",
    "\n",
    "print('Sorting')\n",
    "all_clusters_with_modularity.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Finding the best and pickling')\n",
    "best_cluster = all_clusters_with_modularity[0]\n",
    "print(best_cluster)\n",
    "#pickle.dump(best_cluster, open( \"best_cluster.pkl\", \"wb\" ) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dict where the key is the cluster number and the values\n",
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is: 9085 communities with only 1 member\n",
      "There is: 1507 communities with more than 1 member\n"
     ]
    }
   ],
   "source": [
    "community_size_bigger_than_one = np.zeros(len(community_dict_bigger_than_one))\n",
    "\n",
    "for i,j in enumerate(community_dict_bigger_than_one):\n",
    "    community_size_bigger_than_one[i] = (len(community_dict_bigger_than_one[j])) \n",
    "\n",
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "print('There is:', sum(community_size == 1), 'communities with only 1 member')\n",
    "print('There is:', len(community_dict_bigger_than_one),'communities with more than 1 member')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)# Tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **paper** with their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenized words in each paper\n",
    "if new_df:\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': \"For stereoscopic optical see-through head-mounted display...\n",
    "    abstracts = nx.get_node_attributes(G, 'abstract')\n",
    "    \n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': 'Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration'\n",
    "    titles = nx.get_node_attributes(G, 'title')\n",
    "\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': title + abstract}\n",
    "    paper_dict = {key: titles.get(key, '') + ' ' + abstracts.get(key, '') for key in set(abstracts) | set(titles)}\n",
    "    \n",
    "    # paper_dict_clean['345a2369-8198-46db-8ebb-b3f622b35381'] = ['scalable', 'feature', 'extraction', 'for', 'coarse',\n",
    "    paper_dict_clean = {key:  clean_and_tokenize(text) for key, text in paper_dict.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    pickle.dump(paper_dict_clean, open( \"paper_dict_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"paper_dict_clean.pkl\", 'rb') as f:\n",
    "        paper_dict_clean = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **cluster** with all words from the papers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making\n",
    "if new_df:\n",
    "    community_texts_clean = {new_list: [] for new_list in range(len(community_dict_bigger_than_one))}\n",
    "    \n",
    "    for cluster_id, paper_ids in enumerate(community_dict_bigger_than_one.values()):\n",
    "        for paper_id in paper_ids:  \n",
    "            community_texts_clean[cluster_id].extend(paper_dict_clean[paper_id[0]])          \n",
    "    \n",
    "    #print('Pickling...')\n",
    "    #pickle.dump(community_text_clean, open( \"community_text_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"community_text_clean.pkl\", 'rb') as f:\n",
    "        community_texts_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the dicts into nltk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into nltk format\n",
    "community_text_clean_text = { cluster_id: nltk.Text(text) for cluster_id, text in community_texts_clean.items() } \n",
    "\n",
    "# {'345a2369-8198-46db-8ebb-b3f622b35381': <Text: scalable feature extraction for coarse to fine jpeg...>,\n",
    "paper_dict_clean_text = { paper_id: nltk.Text(text) for paper_id, text in paper_dict_clean.items() } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for each **community**, looking at the 'document' as a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "if new_df:\n",
    "    # TF_clusters[100] = Counter({'the': 28, 'of': 23,'in': 14,'we': 13,\n",
    "    TF_clusters = {}\n",
    "    word_set = set()\n",
    "\n",
    "    for cluster_, text in enumerate(community_text_clean_text.values()):\n",
    "        overall_freq = Counter()           \n",
    "        try:\n",
    "            fd = nltk.FreqDist(text)\n",
    "            word_set.update(set(list(fd.keys())))\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "        except:\n",
    "            print('Breaked')\n",
    "            continue\n",
    "            \n",
    "        TF_clusters[cluster] = overall_freq\n",
    "\n",
    "    pickle.dump(TF_clusters, open( \"TF_clusters.pkl\", \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    with open(\"TF_clusters.pkl\", 'rb') as f:\n",
    "        TF_clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for each **paper** inside their community, looking at the 'document' as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_df:    \n",
    "    TF_papers = {}\n",
    "    word_set = set()\n",
    "\n",
    "    for cluster_, paper_ids in enumerate(community_dict_bigger_than_one.values()): \n",
    "        TF_papers[cluster_] = {}\n",
    "        \n",
    "        for paper_id in paper_ids:         \n",
    "            overall_freq = Counter()     \n",
    "                    \n",
    "            try:\n",
    "                text = paper_dict_clean_text[paper_id[0]]\n",
    "                fd = nltk.FreqDist(text)\n",
    "                word_set.update(set(list(fd.keys())))\n",
    "                overall_freq = overall_freq + Counter(fd)\n",
    "                \n",
    "            except:\n",
    "                print('Breaked')\n",
    "                continue\n",
    "            \n",
    "            TF_papers[cluster_][paper_id[0]] = overall_freq\n",
    "            \n",
    "    pickle.dump(TF_papers, open( \"TF_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"TF_papers.pkl\", 'rb') as f:\n",
    "        TF_papers = pickle.load(f)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word, TF):\n",
    "    occ = sum(1 for i in TF if word in TF[i])\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def tf_idf(number, com, TF):    \n",
    "    total_words = len(TF[number])\n",
    "    return {word: (TF[number][word] / total_words) * idf(word, TF) for word in com}\n",
    "\n",
    "if new_df:\n",
    "    tf_idf_all_communities = [tf_idf(i, TF_clusters.get(i), TF_clusters) for i in TF_clusters]\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_papers(word,papers):\n",
    "    occ = sum(1 for paper in papers.values() if word in paper)\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def tf_idf_papers(cluster, paper_id, counter): \n",
    "    print(paper_id)\n",
    "    total_words = counter.total()\n",
    "    return {word: (counter[word] / total_words) * idf_papers(word, TF_papers[cluster]) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "    tf_idf_all_papers = {}\n",
    "    for cluster_, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        tf_idf_all_papers[cluster_] = [tf_idf_papers(cluster_, paper_id, paper) for paper_id, paper in papers.items()]\n",
    "    #for cluster in tqdm(TF_papers, desc=\"Processing clusters\"):    \n",
    "    #    tf_idf_all_papers[cluster] = [tf_idf_papers(cluster, i, TF_papers.get(cluster)[i]) for i in TF_papers[cluster]]\n",
    "        \n",
    "    pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for user-input. At cluster level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the inverse document frequency for each word\n",
      "Term frequency:  Counter({'the': 1, 'inverse': 1, 'document': 1, 'frequency': 1, 'for': 1, 'each': 1, 'word': 1})\n",
      "TF-IDF scores:  {'the': -0.7159002483770593, 'inverse': -0.2629356619139267, 'document': -0.1944252218765144, 'frequency': -0.4243449236528144, 'for': -0.7053353569037748, 'each': -0.502499691051188, 'word': -0.2270336007309401}\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print(text)\n",
    "\n",
    "def text_cluster(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "    \n",
    "            \n",
    "def text_idf(word):\n",
    "    occ = sum(word in cluster for cluster in TF_clusters.values())\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def text_tf_idf(tf_words):\n",
    "    tf_idf_scores = {} \n",
    "    for word in tf_words:\n",
    "        tf = TF_text[word]/len(TF_text)\n",
    "        idfreq = text_idf(word)\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "\n",
    "print('TF-IDF scores: ', cLevel_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_papers(word,papers):\n",
    "    occ = sum(1 for paper in papers.values() if word in paper)\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def tf_idf_papers(cluster, paper_id, counter): \n",
    "    print(paper_id)\n",
    "    total_words = len(counter)\n",
    "    return {word: (counter[word] / total_words) * idf_papers(word, TF_papers[cluster]) for word in counter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(tf, keywords):\n",
    "    set_input_words = set(tf.keys())\n",
    "    set_keywords = set(keywords)\n",
    "    \n",
    "    intersection = len(set_input_words & set_keywords)\n",
    "    union = len(set_input_words | set_keywords)\n",
    "    \n",
    "    similarity = intersection / union if union != 0 else 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cluster = {}\n",
    "for i in TF_clusters:\n",
    "    sim_cluster[i] = jaccard_similarity(TF_clusters[i], cLevel_tf_idf)\n",
    "    \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_cluster, best_cluster_similarity = sorted_similarity[0]\n",
    "print('The cluster choosen was:', best_cluster, '\\nWith a jaccard similarity of:', best_cluster_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words from the located cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community = tf_idf_all_communities[best_cluster]\n",
    "sorted(community, key=community.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF inside the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_paper(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "    \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf_paper(word):\n",
    "    occ = sum(word in cluster for cluster in TF_papers[best_cluster].values())\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def text_tf_idf_paper(tf_words):\n",
    "    tf_idf_scores = {} \n",
    "    for word in tf_words:\n",
    "        tf = TF_papers[best_cluster][word]/len(TF_papers[best_cluster])\n",
    "        idfreq = text_idf_paper(word)\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "TF_paper = text_paper(text)\n",
    "dLevel_tf_idf = text_tf_idf(TF_paper)\n",
    "print('TF-IDF scores for inside cluster: ', str(best_cluster), ', is ', dLevel_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_paper = {}\n",
    "for i in TF_papers.get(best_cluster):\n",
    "    sim_paper[i] = jaccard_similarity(TF_papers.get(best_cluster)[i], dLevel_tf_idf)\n",
    "    \n",
    "sorted_similarity_paper = sorted(sim_paper.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_papers = sorted_similarity_paper[10]\n",
    "print('The 10 best papers are ', best_papers.keys() , '\\nWith a jaccard similarity of:', best_papers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in best_papers.keys():\n",
    "    print(titles[b])\n",
    "    print(abstracts[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iony's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, paper_:Counter):\n",
    "    return paper_.get(term)/paper_.total()\n",
    "\n",
    "def inv_doc_freq(term_, cluster_:dict):\n",
    "    docs =0 \n",
    "    for _, p_counter in cluster_.items():\n",
    "        if term_ in p_counter:\n",
    "            docs +=1\n",
    "    return np.log(len(cluster_)/docs)\n",
    "\n",
    "tf_idf = lambda term, paper, cluster: term_freq(term, paper) * inv_doc_freq(term, cluster)\n",
    "\n",
    "def paper_tfidf(paper_:Counter, cluster_: dict):\n",
    "    terms_tfidf = Counter()\n",
    "    for term in paper_:\n",
    "        terms_tfidf[term] = tf_idf(term, paper_, cluster_)\n",
    "    return terms_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dummy = {\n",
    "    '01f02fae-97df-4207-a386-a1bc8ec0853b':TF_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b'],\n",
    "    '076bca9b-b30c-4c2c-8148-3b9b5d9aa939':TF_papers[0]['076bca9b-b30c-4c2c-8148-3b9b5d9aa939']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = TF_papers[0]\n",
    "TFIDFs = dict.fromkeys(cluster.keys())\n",
    "for p_id, p_counter in cluster.items():\n",
    "    TFIDFs[p_id] = paper_tfidf(p_counter, cluster)\n",
    "    \n",
    "c0_df = DataFrame(columns=list(TF_clusters[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things we don't think we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_df:\n",
    "    tf_idf_all_papers = {}\n",
    "    for cluster in TF_papers:\n",
    "        tf_idf_all_papers[cluster] = [tf_idf(i, TF_papers.get(cluster)[i], TF_papers.get(cluster)) for i in TF_papers.get(cluster)]\n",
    "        \n",
    "    #pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "if new_df:\n",
    "    tf_idf_all_communities = [tf_idf(i, TF_clusters.get(i), TF_clusters) for i in TF_clusters]\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
