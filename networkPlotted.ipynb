{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if you're starting over\n",
    "new_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if new_df:\n",
    "    # compute the best partition\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # compute modularity\n",
    "    mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "    pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"partition.pkl\", 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    with open(\"mod.pkl\", 'rb') as f:\n",
    "        mod = pickle.load(f)  \n",
    "        \n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_paths(G, root):\n",
    "    shortest_paths_dict = {root: [[root]]}\n",
    "    queue = deque([(root, [root])])\n",
    "\n",
    "    while queue:\n",
    "        s, path = queue.popleft()\n",
    "\n",
    "        for neighbor in G.neighbors(s):\n",
    "            new_path = path + [neighbor]\n",
    "            old_path = shortest_paths_dict.get(neighbor, [[None] * (len(new_path) + 1)])\n",
    "\n",
    "            if len(new_path) == len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor].append(new_path)\n",
    "            elif len(new_path) < len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor] = [new_path]\n",
    "                queue.append((neighbor, new_path))\n",
    "\n",
    "    return shortest_paths_dict\n",
    "\n",
    "def edge_betweenness_centrality(G):\n",
    "    edge_betweenness = defaultdict(float)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        shortest_paths_dict = bfs_shortest_paths(G, node)\n",
    "\n",
    "        for paths in shortest_paths_dict.values():\n",
    "            for path in paths:\n",
    "                for i in range(len(path) - 1):\n",
    "                    edge = (path[i], path[i + 1])\n",
    "                    edge_betweenness[edge] += 1.0\n",
    "\n",
    "    return edge_betweenness\n",
    "\n",
    "def girvan_newman_directed(G):\n",
    "    G_copy = G.copy()\n",
    "    communities = list(nx.weakly_connected_components(G_copy))\n",
    "    results = {0: communities}\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    while G_copy.number_of_edges() > 0:\n",
    "        edge_betweenness = edge_betweenness_centrality(G_copy)\n",
    "        max_betweenness = max(edge_betweenness.values())\n",
    "        highest_betweenness_edges = [edge for edge, value in edge_betweenness.items() if value == max_betweenness]\n",
    "        G_copy.remove_edges_from(highest_betweenness_edges)\n",
    "        components = list(nx.weakly_connected_components(G_copy))\n",
    "        results[step] = components\n",
    "        step += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def modularity(G, clusters_list):\n",
    "    Q = 0\n",
    "    m = len(list(G.edges()))\n",
    "    for aCommunity in clusters_list:\n",
    "        print(\"aCommunity\", aCommunity)\n",
    "        for v in list(aCommunity):\n",
    "            for w in list(aCommunity):\n",
    "                if v != w:\n",
    "                    avw = 1 if (v,w) in list(G.edges()) or (w,v) in list(G.edges()) else 0               \n",
    "                    new_term = avw - (G.degree(v)*G.degree(w))/(2*m)\n",
    "                    Q += new_term\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_modularity_for_all_communities(G, all_communities):\n",
    "    result = []\n",
    "    t = tqdm(total=len(list(all_communities.values())))\n",
    "    for aCommunityRepartition in list(all_communities.values()):\n",
    "        t.update()\n",
    "        aModularity = modularity(G, aCommunityRepartition)\n",
    "        result.append(\n",
    "            [aCommunityRepartition, aModularity]\n",
    "        )\n",
    "    t.close    \n",
    "    return result\n",
    "\n",
    "    \n",
    "B = nx.DiGraph()\n",
    "all_nodes = ['A','B','C','D','E','F','G','H']\n",
    "B.add_nodes_from(all_nodes)\n",
    "all_edges = [\n",
    "    ('E','D'),('E','F'),\n",
    "    ('F','G'),('D','G'),('D','B'),('B','A'),('B','C'),('A','H'),\n",
    "    ('D','F'),('A','C')\n",
    "]\n",
    "B.add_edges_from(all_edges)\n",
    "\n",
    "print('Finding communities...')\n",
    "all_com = girvan_newman_directed(B)\n",
    "\n",
    "print('Finding the modularity...')    \n",
    "all_clusters_with_modularity = compute_modularity_for_all_communities(B, all_com)\n",
    "\n",
    "print('Sorting')\n",
    "all_clusters_with_modularity.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Finding the best and pickling')\n",
    "best_cluster = all_clusters_with_modularity[0]\n",
    "print(best_cluster)\n",
    "#pickle.dump(best_cluster, open( \"best_cluster.pkl\", \"wb\" ) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dict where the key is the cluster number and the values\n",
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is: 9085 communities with only 1 member\n",
      "There is: 1507 communities with more than 1 member\n"
     ]
    }
   ],
   "source": [
    "community_size_bigger_than_one = np.zeros(len(community_dict_bigger_than_one))\n",
    "\n",
    "for i,j in enumerate(community_dict_bigger_than_one):\n",
    "    community_size_bigger_than_one[i] = (len(community_dict_bigger_than_one[j])) \n",
    "\n",
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "print('There is:', sum(community_size == 1), 'communities with only 1 member')\n",
    "print('There is:', len(community_dict_bigger_than_one),'communities with more than 1 member')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)# Tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **paper** with their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickeling...\n"
     ]
    }
   ],
   "source": [
    "## Tokenized words in each paper\n",
    "if new_df:\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': \"For stereoscopic optical see-through head-mounted display...\n",
    "    abstracts = nx.get_node_attributes(G, 'abstract')\n",
    "    \n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': 'Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration'\n",
    "    titles = nx.get_node_attributes(G, 'title')\n",
    "\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': title + abstract}\n",
    "    paper_dict = {key: titles.get(key, '') + ' ' + abstracts.get(key, '') for key in set(abstracts) | set(titles)}\n",
    "    \n",
    "    # paper_dict_clean['345a2369-8198-46db-8ebb-b3f622b35381'] = ['scalable', 'feature', 'extraction', 'for', 'coarse',\n",
    "    paper_dict_clean = {key:  clean_and_tokenize(text) for key, text in paper_dict.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    pickle.dump(paper_dict_clean, open( \"paper_dict_clean.pkl\", \"wb\" ) )\n",
    "    pickle.dump(abstracts, open( \"abstracts.pkl\", \"wb\" ) )\n",
    "    pickle.dump(titles, open( \"titles.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open(\"paper_dict_clean.pkl\", 'rb') as f:\n",
    "        paper_dict_clean = pickle.load(f)\n",
    "    \n",
    "    with open(\"abstracts.pkl\", 'rb') as f:\n",
    "        abstracts = pickle.load(f)\n",
    "    \n",
    "    with open(\"titles.pkl\", 'rb') as f:\n",
    "        titles = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **cluster** with all words from the papers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if new_df:\n",
    "    community_texts_clean = {new_list: [] for new_list in range(len(community_dict_bigger_than_one))}\n",
    "    \n",
    "    for cluster_id, paper_ids in enumerate(community_dict_bigger_than_one.values()):\n",
    "        for paper_id in paper_ids:  \n",
    "            community_texts_clean[cluster_id].extend(paper_dict_clean[paper_id[0]])          \n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(community_text_clean, open( \"community_text_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"community_text_clean.pkl\", 'rb') as f:\n",
    "        community_texts_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the dicts into nltk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into nltk format\n",
    "community_text_clean_text = { cluster_id: nltk.Text(text) for cluster_id, text in community_texts_clean.items() } \n",
    "\n",
    "# {'345a2369-8198-46db-8ebb-b3f622b35381': <Text: scalable feature extraction for coarse to fine jpeg...>,\n",
    "paper_dict_clean_text = { paper_id: nltk.Text(text) for paper_id, text in paper_dict_clean.items() } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **community**, looking at the 'document' as a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "if new_df:\n",
    "    \n",
    "    # TF_clusters[100] = Counter({'the': 28, 'of': 23,'in': 14,'we': 13,\n",
    "    TF_clusters = {}\n",
    "\n",
    "    for cluster_, text in enumerate(community_text_clean_text.values()):\n",
    "        overall_freq = Counter()           \n",
    "        try:\n",
    "            fd = nltk.FreqDist(text)\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "        except:\n",
    "            print('Breaked')\n",
    "            continue\n",
    "            \n",
    "        TF_clusters[cluster_] = overall_freq\n",
    "\n",
    "    pickle.dump(TF_clusters, open( \"TF_clusters.pkl\", \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    with open(\"TF_clusters.pkl\", 'rb') as f:\n",
    "        TF_clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **paper** inside their community, looking at the 'document' as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_df:    \n",
    "    TF_papers = {}\n",
    "\n",
    "    for cluster_, paper_ids in enumerate(community_dict_bigger_than_one.values()): \n",
    "        TF_papers[cluster_] = {}\n",
    "        \n",
    "        for paper_id in paper_ids:         \n",
    "            overall_freq = Counter()     \n",
    "                    \n",
    "            try:\n",
    "                text = paper_dict_clean_text[paper_id[0]]\n",
    "                fd = nltk.FreqDist(text)\n",
    "                overall_freq = overall_freq + Counter(fd)\n",
    "                \n",
    "            except:\n",
    "                print('Breaked')\n",
    "                continue\n",
    "            \n",
    "            TF_papers[cluster_][paper_id[0]] = overall_freq\n",
    "            \n",
    "    pickle.dump(TF_papers, open( \"TF_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"TF_papers.pkl\", 'rb') as f:\n",
    "        TF_papers = pickle.load(f)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "new_df = True\n",
    "\n",
    "word_count = lambda word: sum(1 for i in TF_clusters if word in TF_clusters[i])\n",
    "\n",
    "def tf_idf(cluster_id, counter):  \n",
    "    # Total number of words in the cluster\n",
    "    total_words = counter.total()\n",
    "    \n",
    "    return {word: (TF_clusters[cluster_id][word] / total_words) * np.log( N / word_count(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "\n",
    "    # Number of clusters\n",
    "    N = len(TF_clusters)\n",
    "    \n",
    "    tf_idf_all_communities = [tf_idf(i, TF_clusters[i]) for i in TF_clusters]\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['performance', 'memory', 'parallel', 'scheduling']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community = tf_idf_all_communities[6]\n",
    "sorted(community, key=community.get, reverse=True)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF for each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1507/1507 [02:43<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if new_df:\n",
    "    \n",
    "    # Dict to store the word counts\n",
    "    word_count_papers = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i in TF_papers:\n",
    "        for paper in TF_papers[i].values():\n",
    "            for word in paper:\n",
    "                word_count_papers[i][word] += 1\n",
    "\n",
    "    tf_idf_all_papers = {}\n",
    "    \n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        # Number of papers inside the current cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "\n",
    "        def tf_idf_papers(counter): \n",
    "\n",
    "            # Total words in the current paper\n",
    "            total_words = counter.total()\n",
    "\n",
    "            return {word: (counter[word] / total_words) * np.log( N / word_count_papers[cluster_id][word] ) for word in counter}\n",
    "\n",
    "        tf_idf_all_papers[cluster_id] = {paper_id_: tf_idf_papers(counter_) for paper_id_, counter_ in papers.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    #pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stereoscopic', 'physical', 'unreliable', 'calibration', 'user']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community = tf_idf_all_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b']\n",
    "sorted(community, key=community.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0: {'01f02fae-97df-4207-a386-a1bc8ec0853b': Counter({'the': 12,'for': 4,'user': 4,\n",
    "# TF_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for user-input. At cluster level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input:  *Evaluating the Efficacy of Renewable Energy Sources in Reducing Carbon Emissions*    This study seeks to investigate the role of various renewable energy sources, such as solar, wind, and hydro, in mitigating carbon emissions, comparing their effectiveness to traditional fossil fuels. The research will cover energy output comparisons between renewable and non-renewable sources, analyze the lifecycle carbon footprint of these energy systems, and assess technological advancements and economic feasibility. Additionally, the impact of relevant policies and regulations on the adoption of renewable energy will be examined.\n",
      "TF-IDF scores:  {'evaluating': 0.03673247808449794, 'the': 0.0002783279297465243, 'efficacy': 0.04219763656409952, 'of': 0.0003482578226355083, 'renewable': 0.21875163588006288, 'energy': 0.12321971903259707, 'sources': 0.10707142871780709, 'in': 0.0006732210257032231, 'reducing': 0.034888233547560286, 'carbon': 0.11880883232087265, 'emissions': 0.08684551767838106, 'this': 0.0015935465067667403, 'study': 0.012736045704129381, 'seeks': 0.04516149993187615, 'to': 0.0009448481859305002, 'investigate': 0.0258619302072062, 'role': 0.027465687686339935, 'various': 0.021363224131215164, 'such': 0.01140756357534947, 'as': 0.004618659236951304, 'solar': 0.041910513161885904, 'wind': 0.04163030907375729, 'and': 0.0008158705435343573, 'hydro': 0.04863234863601564, 'mitigating': 0.047206740239593176, 'comparing': 0.03473421928302516, 'their': 0.01405382111467593, 'effectiveness': 0.03329684770201148, 'traditional': 0.030756364699588246, 'fossil': 0.047206740239593176, 'fuels': 0.047206740239593176, 'research': 0.023658982208005892, 'will': 0.04196389424329101, 'cover': 0.03937268099886244, 'output': 0.028672627147754398, 'comparisons': 0.03807935966037145, 'between': 0.012484991836776272, 'non': 0.019202901285047248, 'analyze': 0.02971746818405511, 'lifecycle': 0.047206740239593176, 'footprint': 0.04593698103069792, 'these': 0.010067758653363599, 'systems': 0.01648812610516284, 'assess': 0.03329684770201148, 'technological': 0.03870804827979327, 'advancements': 0.04554277694144581, 'economic': 0.03473421928302516, 'feasibility': 0.03870804827979327, 'additionally': 0.0371016489951326, 'impact': 0.02952038029668565, 'relevant': 0.03142739164866554, 'policies': 0.04163030907375729, 'regulations': 0.04443451903674436, 'on': 0.0031538246602062768, 'adoption': 0.04163030907375729, 'be': 0.005003763104207881, 'examined': 0.03142739164866554}\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print('The user input: ', text)\n",
    "\n",
    "def text_cluster(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf(word):\n",
    "    return max(1, sum(word in cluster for cluster in TF_clusters.values()))\n",
    "\n",
    "def text_tf_idf(tf_words):\n",
    "    tf_idf_scores = {}\n",
    "    N = len(TF_clusters) \n",
    "    for word in tf_words:\n",
    "        tf = TF_text[word]/TF_text.total()\n",
    "        idfreq = np.log( N / text_idf(word) )\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "\n",
    "print('TF-IDF scores: ', cLevel_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(tf, keywords):\n",
    "    set_input_words = set(tf.keys())\n",
    "    set_keywords = set(keywords)\n",
    "    \n",
    "    intersection = len(set_input_words & set_keywords)\n",
    "    union = len(set_input_words | set_keywords)\n",
    "    \n",
    "    similarity = intersection / union if union != 0 else 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cluster choosen was: 53 \n",
      "With a jaccard similarity of: 0.12345679012345678\n"
     ]
    }
   ],
   "source": [
    "sim_cluster = {}\n",
    "for i in TF_clusters:\n",
    "    sim_cluster[i] = jaccard_similarity(TF_clusters[i], cLevel_tf_idf)\n",
    "    \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_cluster, best_cluster_similarity = sorted_similarity[0]\n",
    "print('The cluster choosen was:', best_cluster, '\\nWith a jaccard similarity of:', best_cluster_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words from the located cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turbines',\n",
       " 'hydro',\n",
       " 'feedback',\n",
       " 'control',\n",
       " 'nonlinear',\n",
       " 'hydraulic',\n",
       " 'controller',\n",
       " 'locally',\n",
       " 'electric',\n",
       " 'computed',\n",
       " 'controlled',\n",
       " 'law',\n",
       " 'strategy',\n",
       " 'power',\n",
       " 'system',\n",
       " 'minimum',\n",
       " 'phase',\n",
       " 'turbine',\n",
       " 'pid',\n",
       " 'proposed']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community = tf_idf_all_communities[best_cluster]\n",
    "sorted(community, key=community.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF inside the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores for inside cluster:  53 , is  {'evaluating': 0.00805985093674355, 'the': 0.0, 'efficacy': 0.00805985093674355, 'of': 0.0, 'renewable': 0.04029925468371775, 'energy': 0.04029925468371775, 'sources': 0.02417955281023065, 'in': 0.0, 'reducing': 0.00805985093674355, 'carbon': 0.02417955281023065, 'emissions': 0.0161197018734871, 'this': 0.0, 'study': 0.00805985093674355, 'seeks': 0.00805985093674355, 'to': 0.0, 'investigate': 0.00805985093674355, 'role': 0.00805985093674355, 'various': 0.00805985093674355, 'such': 0.00805985093674355, 'as': 0.00805985093674355, 'solar': 0.00805985093674355, 'wind': 0.00805985093674355, 'and': 0.0, 'hydro': 0.00805985093674355, 'mitigating': 0.00805985093674355, 'comparing': 0.00805985093674355, 'their': 0.00805985093674355, 'effectiveness': 0.00805985093674355, 'traditional': 0.00805985093674355, 'fossil': 0.00805985093674355, 'fuels': 0.00805985093674355, 'research': 0.00805985093674355, 'will': 0.0161197018734871, 'cover': 0.00805985093674355, 'output': 0.00805985093674355, 'comparisons': 0.00805985093674355, 'between': 0.00805985093674355, 'non': 0.00805985093674355, 'analyze': 0.00805985093674355, 'lifecycle': 0.00805985093674355, 'footprint': 0.00805985093674355, 'these': 0.00805985093674355, 'systems': 0.00805985093674355, 'assess': 0.00805985093674355, 'technological': 0.00805985093674355, 'advancements': 0.00805985093674355, 'economic': 0.00805985093674355, 'feasibility': 0.00805985093674355, 'additionally': 0.00805985093674355, 'impact': 0.00805985093674355, 'relevant': 0.00805985093674355, 'policies': 0.00805985093674355, 'regulations': 0.00805985093674355, 'on': 0.00805985093674355, 'adoption': 0.00805985093674355, 'be': 0.00805985093674355, 'examined': 0.00805985093674355}\n"
     ]
    }
   ],
   "source": [
    "def text_paper(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf_paper(word):\n",
    "    return max(1,sum(word in cluster for cluster in TF_papers[best_cluster].values()))\n",
    "\n",
    "def text_tf_idf_paper(tf_words, best_cluster):\n",
    "    N = len(TF_papers[best_cluster])\n",
    "    tf_idf_scores = {} \n",
    "    for word in tf_words:\n",
    "        tf = TF_paper[word]/TF_paper.total()\n",
    "        idfreq = np.log(N / text_idf_paper(word))\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "TF_paper = text_paper(text)\n",
    "dLevel_tf_idf = text_tf_idf_paper(TF_paper, best_cluster)\n",
    "print('TF-IDF scores for inside cluster: ', str(best_cluster), ', is ', dLevel_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf_all_papers[best_cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###   Papers with highest jaccard similarity   ###\n",
      "\n",
      "( Paper id, jaccard similarity score) \n",
      "('d0ebbecd-04e1-415f-96ed-7d3146197b59', 0.12)\n",
      "('4f947c17-08d1-49cc-be5a-cb298730da5a', 0.09401709401709402)\n"
     ]
    }
   ],
   "source": [
    "sim_paper = {}\n",
    "for i in TF_papers[best_cluster]:\n",
    "    sim_paper[i] = jaccard_similarity(TF_papers[best_cluster][i], dLevel_tf_idf)\n",
    "  \n",
    "sorted_similarity_paper = sorted(sim_paper.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "best_papers = sorted_similarity_paper[:10]\n",
    "print('\\n###   Papers with highest jaccard similarity   ###\\n')\n",
    "print('( Paper id, jaccard similarity score) ')\n",
    "for paper in best_papers:\n",
    "    print(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:  Control of hydraulic turbine generators using exact feedback linearization\n",
      "Abstract:  Feedback control of hydro turbines in power systems is important for the transient stability enhancement. However, the hydro-electric power system is hard to be controlled due to its inherent non-minimum phase property. While global control of such a non-minimum phase system remains a challenging problem, it is shown in this paper that locally, the hydro-electric power plant is feedback equivalent to a linear and controllable system, and hence can be locally controlled, in the new coordinates, by using a linear feedback design technique such as pole assignment. Simulation results in a single machine infinite bus system are provided to illustrate the effectiveness of the proposed nonlinear control scheme. Performance comparisons between a PID controller and the nonlinear controller are also given in the paper.\n",
      "\n",
      "Title:  Hydraulic-turbine start-up with “S-shaped” characteristic\n",
      "Abstract:  Fast response of hydraulic turbines is a key condition for the integration of renewable sources of energy. In this paper, a new start-up strategy is proposed for hydraulic turbines prone to “S” instability. This method is based on a complete nonlinear model of the turbine together with the upstream water pipe. More precisely, the control strategy is based on a gain scheduling approach that is computed using finite horizon predictive control. This yields a state feedback control law that tracks a time-optimal trajectory that is computed based on the nonlinear model of the system. Simulations are shown to assess the efficiency of the proposed law and its robustness to model discrepancies.\n"
     ]
    }
   ],
   "source": [
    "for b in best_papers:\n",
    "    print('\\nTitle: ',titles[b[0]])\n",
    "    print('Abstract: ', abstracts[b[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input:  birds butterflies frogs sun plants\n",
      "The cluster choosen was: 1251 \n",
      "With a jaccard similarity of: 0.014492753623188406 \n",
      "\n",
      "\n",
      "###   Papers with highest jaccard similarity   ###\n",
      "\n",
      "( Paper id, jaccard similarity score) \n",
      "('1bf1acfd-1fe0-4b1e-a652-dc40ac4c92ca', 0.02702702702702703)\n",
      "('e5b2f2f2-d71c-41d0-bbb5-35fa37b32236', 0.0)\n",
      "\n",
      "###   Papers recommended   ###\n",
      "\n",
      "Title:  On perturbation bounds of generalized eigenvalues for diagonalizable pairs\n",
      "Abstract:  The purpose of this paper is to study the perturbation of generalized eigenvalues. Two perturbation bounds of the diagonalizable pairs are given. These results extend the corresponding ones given by Sun (Math Numer Sinica 4:23–29, 1982).\n",
      "\n",
      "Title:  On perturbations of matrix pencils with real spectra, a revisit\n",
      "Abstract:  This paper continues earlier studies by Bhatia and Li on eigenvalue perturbation theory for diagonalizable matrix pencils having real spectra. A unifying framework for creating crucial perturbation equations is developed. With the help of a recent result on generalized commutators involving unitary matrices, new and much sharper bounds are obtained.\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print('The user input: ', text)\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "\n",
    "# Similarity on cluster-level\n",
    "sim_cluster = {}\n",
    "for i in TF_clusters:\n",
    "    sim_cluster[i] = jaccard_similarity(TF_clusters[i], cLevel_tf_idf)\n",
    "    \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_cluster, best_cluster_similarity = sorted_similarity[0]\n",
    "print('The cluster choosen was:', best_cluster, '\\nWith a jaccard similarity of:', best_cluster_similarity, '\\n')\n",
    "\n",
    "# Similarity on paper-level\n",
    "TF_paper = text_paper(text)\n",
    "dLevel_tf_idf = text_tf_idf_paper(TF_paper, best_cluster)\n",
    "\n",
    "sim_paper = {}\n",
    "for i in TF_papers[best_cluster]:\n",
    "    sim_paper[i] = jaccard_similarity(TF_papers[best_cluster][i], dLevel_tf_idf)\n",
    "  \n",
    "sorted_similarity_paper = sorted(sim_paper.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "best_papers = sorted_similarity_paper[:10]\n",
    "\n",
    "print('\\n###   Papers with highest jaccard similarity   ###\\n')\n",
    "print('( Paper id, jaccard similarity score) ')\n",
    "for paper in best_papers:\n",
    "    print(paper)\n",
    "\n",
    "print('\\n###   Papers recommended   ###')\n",
    "for b in best_papers:\n",
    "    print('\\nTitle: ',titles[b[0]])\n",
    "    print('Abstract: ', abstracts[b[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iony's work work work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, paper_:Counter):\n",
    "    return paper_.get(term)/paper_.total()\n",
    "\n",
    "def inv_doc_freq(term_, cluster_:dict):\n",
    "    N = len(cluster_)\n",
    "    docs =0\n",
    "    for _, p_counter in cluster_.items():\n",
    "        if term_ in p_counter:\n",
    "            docs +=1\n",
    "    docs_w_term = tuple((p_id,p_counter) for  p_id, p_counter in cluster_.items() if term_ in p_counter.keys())\n",
    "    return np.log(N/len(docs_w_term))\n",
    "\n",
    "tf_idf = lambda term, paper, cluster: term_freq(term, paper) * inv_doc_freq(term, cluster)\n",
    "\n",
    "def paper_tfidf(paper_:Counter, cluster_: dict):\n",
    "    terms_tfidf = Counter()\n",
    "    for term in paper_:\n",
    "        terms_tfidf[term] = tf_idf(term, paper_, cluster_)\n",
    "    return terms_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01f02fae-97df-4207-a386-a1bc8ec0853b\n",
      "076bca9b-b30c-4c2c-8148-3b9b5d9aa939\n"
     ]
    }
   ],
   "source": [
    "cluster0_keys = TF_papers[0].keys()\n",
    "\n",
    "print(list(cluster0_keys)[0])\n",
    "print(list(cluster0_keys)[1])\n",
    "print(list(cluster0_keys)[2])\n",
    "\n",
    "cluster_dummy = {\n",
    "    '01f02fae-97df-4207-a386-a1bc8ec0853b':TF_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b'],\n",
    "    '076bca9b-b30c-4c2c-8148-3b9b5d9aa939':TF_papers[0]['076bca9b-b30c-4c2c-8148-3b9b5d9aa939']\n",
    "}\n",
    "\n",
    "cluster = TF_papers[0]\n",
    "TFIDFs = dict.fromkeys(cluster.keys())\n",
    "for p_id, p_counter in cluster.items():\n",
    "    TFIDFs[p_id] = paper_tfidf(p_counter, cluster)\n",
    "    \n",
    "c0_df = DataFrame(columns=list(TF_clusters[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_papers(word,papers):\n",
    "    occ = sum(1 for paper in papers.values() if word in paper)\n",
    "    return np.log(10 / occ)\n",
    "\n",
    "def tf_idf_papers(cluster, paper_id, counter): \n",
    "    print(paper_id)\n",
    "    total_words = len(counter)\n",
    "    return {word: (counter[word] / total_words) * idf_papers(word, TF_papers[cluster]) for word in counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = True\n",
    "\n",
    "word_count_papers = lambda word: sum(1 for i in papers if word in papers[i])\n",
    "\n",
    "def tf_idf_papers(paper_id, counter): \n",
    "    print(paper_id)\n",
    "    \n",
    "    # Total words in the current paper\n",
    "    total_words = counter.total()\n",
    "       \n",
    "    return {word: (counter[word] / total_words) * np.log( N / word_count_papers(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "    tf_idf_all_papers = {}\n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        \n",
    "        # Number of papers inside the currect cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "        \n",
    "        tf_idf_all_papers[cluster_id] = [tf_idf_papers(paper_id_, counter_) for paper_id_, counter_ in papers.items()]\n",
    "    \n",
    "    pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
