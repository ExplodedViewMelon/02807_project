{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if you're starting over\n",
    "new_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if new_df:\n",
    "    # compute the best partition\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # compute modularity\n",
    "    mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "    pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"partition.pkl\", 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    with open(\"mod.pkl\", 'rb') as f:\n",
    "        mod = pickle.load(f)  \n",
    "        \n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_paths(G, root):\n",
    "    shortest_paths_dict = {root: [[root]]}\n",
    "    queue = deque([(root, [root])])\n",
    "\n",
    "    while queue:\n",
    "        s, path = queue.popleft()\n",
    "\n",
    "        for neighbor in G.neighbors(s):\n",
    "            new_path = path + [neighbor]\n",
    "            old_path = shortest_paths_dict.get(neighbor, [[None] * (len(new_path) + 1)])\n",
    "\n",
    "            if len(new_path) == len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor].append(new_path)\n",
    "            elif len(new_path) < len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor] = [new_path]\n",
    "                queue.append((neighbor, new_path))\n",
    "\n",
    "    return shortest_paths_dict\n",
    "\n",
    "def edge_betweenness_centrality(G):\n",
    "    edge_betweenness = defaultdict(float)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        shortest_paths_dict = bfs_shortest_paths(G, node)\n",
    "\n",
    "        for paths in shortest_paths_dict.values():\n",
    "            for path in paths:\n",
    "                for i in range(len(path) - 1):\n",
    "                    edge = (path[i], path[i + 1])\n",
    "                    edge_betweenness[edge] += 1.0\n",
    "\n",
    "    return edge_betweenness\n",
    "\n",
    "def girvan_newman_directed(G):\n",
    "    G_copy = G.copy()\n",
    "    communities = list(nx.weakly_connected_components(G_copy))\n",
    "    results = {0: communities}\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    while G_copy.number_of_edges() > 0:\n",
    "        edge_betweenness = edge_betweenness_centrality(G_copy)\n",
    "        max_betweenness = max(edge_betweenness.values())\n",
    "        highest_betweenness_edges = [edge for edge, value in edge_betweenness.items() if value == max_betweenness]\n",
    "        G_copy.remove_edges_from(highest_betweenness_edges)\n",
    "        components = list(nx.weakly_connected_components(G_copy))\n",
    "        results[step] = components\n",
    "        step += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def modularity(G, clusters_list):\n",
    "    Q = 0\n",
    "    m = len(list(G.edges()))\n",
    "    for aCommunity in clusters_list:\n",
    "        print(\"aCommunity\", aCommunity)\n",
    "        for v in list(aCommunity):\n",
    "            for w in list(aCommunity):\n",
    "                if v != w:\n",
    "                    avw = 1 if (v,w) in list(G.edges()) or (w,v) in list(G.edges()) else 0               \n",
    "                    new_term = avw - (G.degree(v)*G.degree(w))/(2*m)\n",
    "                    Q += new_term\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_modularity_for_all_communities(G, all_communities):\n",
    "    result = []\n",
    "    t = tqdm(total=len(list(all_communities.values())))\n",
    "    for aCommunityRepartition in list(all_communities.values()):\n",
    "        t.update()\n",
    "        aModularity = modularity(G, aCommunityRepartition)\n",
    "        result.append(\n",
    "            [aCommunityRepartition, aModularity]\n",
    "        )\n",
    "    t.close    \n",
    "    return result\n",
    "\n",
    "    \n",
    "B = nx.DiGraph()\n",
    "all_nodes = ['A','B','C','D','E','F','G','H']\n",
    "B.add_nodes_from(all_nodes)\n",
    "all_edges = [\n",
    "    ('E','D'),('E','F'),\n",
    "    ('F','G'),('D','G'),('D','B'),('B','A'),('B','C'),('A','H'),\n",
    "    ('D','F'),('A','C')\n",
    "]\n",
    "B.add_edges_from(all_edges)\n",
    "\n",
    "print('Finding communities...')\n",
    "all_com = girvan_newman_directed(B)\n",
    "\n",
    "print('Finding the modularity...')    \n",
    "all_clusters_with_modularity = compute_modularity_for_all_communities(B, all_com)\n",
    "\n",
    "print('Sorting')\n",
    "all_clusters_with_modularity.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Finding the best and pickling')\n",
    "best_cluster = all_clusters_with_modularity[0]\n",
    "print(best_cluster)\n",
    "#pickle.dump(best_cluster, open( \"best_cluster.pkl\", \"wb\" ) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dict where the key is the cluster number and the values\n",
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is: 9085 communities with only 1 member\n",
      "There is: 1507 communities with more than 1 member\n"
     ]
    }
   ],
   "source": [
    "community_size_bigger_than_one = np.zeros(len(community_dict_bigger_than_one))\n",
    "\n",
    "for i,j in enumerate(community_dict_bigger_than_one):\n",
    "    community_size_bigger_than_one[i] = (len(community_dict_bigger_than_one[j])) \n",
    "\n",
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "print('There is:', sum(community_size == 1), 'communities with only 1 member')\n",
    "print('There is:', len(community_dict_bigger_than_one),'communities with more than 1 member')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)# Tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **paper** with their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenized words in each paper\n",
    "if new_df:\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': \"For stereoscopic optical see-through head-mounted display...\n",
    "    abstracts = nx.get_node_attributes(G, 'abstract')\n",
    "    \n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': 'Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration'\n",
    "    titles = nx.get_node_attributes(G, 'title')\n",
    "\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': title + abstract}\n",
    "    paper_dict = {key: titles.get(key, '') + ' ' + abstracts.get(key, '') for key in set(abstracts) | set(titles)}\n",
    "    \n",
    "    # paper_dict_clean['345a2369-8198-46db-8ebb-b3f622b35381'] = ['scalable', 'feature', 'extraction', 'for', 'coarse',\n",
    "    paper_dict_clean = {key:  clean_and_tokenize(text) for key, text in paper_dict.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    pickle.dump(paper_dict_clean, open( \"paper_dict_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"paper_dict_clean.pkl\", 'rb') as f:\n",
    "        paper_dict_clean = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **cluster** with all words from the papers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if new_df:\n",
    "    community_texts_clean = {new_list: [] for new_list in range(len(community_dict_bigger_than_one))}\n",
    "    \n",
    "    for cluster_id, paper_ids in enumerate(community_dict_bigger_than_one.values()):\n",
    "        for paper_id in paper_ids:  \n",
    "            community_texts_clean[cluster_id].extend(paper_dict_clean[paper_id[0]])          \n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(community_text_clean, open( \"community_text_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"community_text_clean.pkl\", 'rb') as f:\n",
    "        community_texts_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the dicts into nltk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into nltk format\n",
    "community_text_clean_text = { cluster_id: nltk.Text(text) for cluster_id, text in community_texts_clean.items() } \n",
    "\n",
    "# {'345a2369-8198-46db-8ebb-b3f622b35381': <Text: scalable feature extraction for coarse to fine jpeg...>,\n",
    "paper_dict_clean_text = { paper_id: nltk.Text(text) for paper_id, text in paper_dict_clean.items() } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **community**, looking at the 'document' as a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "if new_df:\n",
    "    \n",
    "    # TF_clusters[100] = Counter({'the': 28, 'of': 23,'in': 14,'we': 13,\n",
    "    TF_clusters = {}\n",
    "\n",
    "    for cluster_, text in enumerate(community_text_clean_text.values()):\n",
    "        overall_freq = Counter()           \n",
    "        try:\n",
    "            fd = nltk.FreqDist(text)\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "        except:\n",
    "            print('Breaked')\n",
    "            continue\n",
    "            \n",
    "        TF_clusters[cluster_] = overall_freq\n",
    "\n",
    "    pickle.dump(TF_clusters, open( \"TF_clusters.pkl\", \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    with open(\"TF_clusters.pkl\", 'rb') as f:\n",
    "        TF_clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **paper** inside their community, looking at the 'document' as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_df:    \n",
    "    TF_papers = {}\n",
    "\n",
    "    for cluster_, paper_ids in enumerate(community_dict_bigger_than_one.values()): \n",
    "        TF_papers[cluster_] = {}\n",
    "        \n",
    "        for paper_id in paper_ids:         \n",
    "            overall_freq = Counter()     \n",
    "                    \n",
    "            try:\n",
    "                text = paper_dict_clean_text[paper_id[0]]\n",
    "                fd = nltk.FreqDist(text)\n",
    "                overall_freq = overall_freq + Counter(fd)\n",
    "                \n",
    "            except:\n",
    "                print('Breaked')\n",
    "                continue\n",
    "            \n",
    "            TF_papers[cluster_][paper_id[0]] = overall_freq\n",
    "            \n",
    "    pickle.dump(TF_papers, open( \"TF_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"TF_papers.pkl\", 'rb') as f:\n",
    "        TF_papers = pickle.load(f)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "new_df = True\n",
    "\n",
    "word_count = lambda word: sum(1 for i in TF_clusters if word in TF_clusters[i])\n",
    "\n",
    "def tf_idf(cluster_id, counter):  \n",
    "    # Total number of words in the cluster\n",
    "    total_words = counter.total()\n",
    "    \n",
    "    return {word: (TF_clusters[cluster_id][word] / total_words) * np.log( N / word_count(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "\n",
    "    # Number of clusters\n",
    "    N = len(TF_clusters)\n",
    "    \n",
    "    tf_idf_all_communities = [tf_idf(i, TF_clusters[i]) for i in TF_clusters]\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['performance', 'memory', 'parallel', 'scheduling']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community = tf_idf_all_communities[6]\n",
    "sorted(community, key=community.get, reverse=True)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF for each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = True\n",
    "\n",
    "word_count_papers = lambda word: sum(1 for i in papers if word in papers[i])\n",
    "\n",
    "def tf_idf_papers(paper_id, counter): \n",
    "    print(paper_id)\n",
    "    \n",
    "    # Total words in the current paper\n",
    "    total_words = counter.total()\n",
    "       \n",
    "    return {word: (counter[word] / total_words) * np.log( N / word_count_papers(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "    tf_idf_all_papers = {}\n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        \n",
    "        # Number of papers inside the currect cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "        \n",
    "        tf_idf_all_papers[cluster_id] = [tf_idf_papers(paper_id_, counter_) for paper_id_, counter_ in papers.items()]\n",
    "    \n",
    "    pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|███████| 1507/1507 [02:34<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if new_df:\n",
    "    \n",
    "    # Dict to store the word counts\n",
    "    word_count_papers = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i in TF_papers:\n",
    "        for paper in TF_papers[i].values():\n",
    "            for word in paper:\n",
    "                word_count_papers[i][word] += 1\n",
    "\n",
    "    tf_idf_all_papers = {}\n",
    "    \n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        # Number of papers inside the current cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "\n",
    "        def tf_idf_papers(counter): \n",
    "\n",
    "            # Total words in the current paper\n",
    "            total_words = counter.total()\n",
    "\n",
    "            return {word: (counter[word] / total_words) * np.log( N / word_count_papers[cluster_id][word] ) for word in counter}\n",
    "\n",
    "        tf_idf_all_papers[cluster_id] = {paper_id_: tf_idf_papers(counter_) for paper_id_, counter_ in papers.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    #pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stereoscopic', 'physical', 'unreliable', 'calibration', 'user']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community = tf_idf_all_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b']\n",
    "sorted(community, key=community.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0: {'01f02fae-97df-4207-a386-a1bc8ec0853b': Counter({'the': 12,'for': 4,'user': 4,\n",
    "# TF_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for user-input. At cluster level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input:      N = len(TF_clusters)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/zhome/5e/c/137246/02807_project/networkPlotted.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m TF_text \u001b[39m=\u001b[39m text_cluster(text)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# TF-IDF for the input text on cluster level\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m cLevel_tf_idf \u001b[39m=\u001b[39m text_tf_idf(TF_text)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTF-IDF scores: \u001b[39m\u001b[39m'\u001b[39m, cLevel_tf_idf)\n",
      "\u001b[1;32m/zhome/5e/c/137246/02807_project/networkPlotted.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tf_words:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     tf \u001b[39m=\u001b[39m TF_text[word]\u001b[39m/\u001b[39mTF_text\u001b[39m.\u001b[39mtotal()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     idfreq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog( N\u001b[39m/\u001b[39;49m text_idf(word) )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     tf_idf_scores[word] \u001b[39m=\u001b[39m tf\u001b[39m*\u001b[39midfreq\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.hpc.dtu.dk/zhome/5e/c/137246/02807_project/networkPlotted.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tf_idf_scores\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print('The user input: ', text)\n",
    "\n",
    "def text_cluster(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf(word):\n",
    "    return max(1, sum(word in cluster for cluster in TF_clusters.values()))\n",
    "\n",
    "def text_tf_idf(tf_words):\n",
    "    tf_idf_scores = {}\n",
    "    N = len(TF_clusters) \n",
    "    for word in tf_words:\n",
    "        tf = TF_text[word]/TF_text.total()\n",
    "        idfreq = np.log( N / text_idf(word) )\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "\n",
    "print('TF-IDF scores: ', cLevel_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(tf, keywords):\n",
    "    set_input_words = set(tf.keys())\n",
    "    set_keywords = set(keywords)\n",
    "    \n",
    "    intersection = len(set_input_words & set_keywords)\n",
    "    union = len(set_input_words | set_keywords)\n",
    "    \n",
    "    similarity = intersection / union if union != 0 else 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cluster = {}\n",
    "for i in TF_clusters:\n",
    "    sim_cluster[i] = jaccard_similarity(TF_clusters[i], cLevel_tf_idf)\n",
    "    \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_cluster, best_cluster_similarity = sorted_similarity[0]\n",
    "print('The cluster choosen was:', best_cluster, '\\nWith a jaccard similarity of:', best_cluster_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words from the located cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community = tf_idf_all_communities[best_cluster]\n",
    "sorted(community, key=community.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF inside the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_paper(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf_paper(word):\n",
    "    occ = sum(word in cluster for cluster in TF_papers[best_cluster].values())\n",
    "    return np.log(10 / max(1, occ))\n",
    "\n",
    "def text_tf_idf_paper(tf_words, b)\n",
    "    N = len(TF_clusters):\n",
    "    tf_idf_scores = {} \n",
    "    for word in tf_words:\n",
    "        tf = TF_papers[best_cluster][word]/len(TF_papers[best_cluster])\n",
    "        idfreq = text_idf_paper(word)\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "TF_paper = text_paper(text)\n",
    "dLevel_tf_idf = text_tf_idf(TF_paper)\n",
    "print('TF-IDF scores for inside cluster: ', str(best_cluster), ', is ', dLevel_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_paper = {}\n",
    "for i in TF_papers.get(best_cluster):\n",
    "    sim_paper[i] = jaccard_similarity(TF_papers.get(best_cluster)[i], dLevel_tf_idf)\n",
    "    \n",
    "sorted_similarity_paper = sorted(sim_paper.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "best_papers = sorted_similarity_paper[10]\n",
    "print('The 10 best papers are ', best_papers.keys() , '\\nWith a jaccard similarity of:', best_papers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in best_papers.keys():\n",
    "    print(titles[b])\n",
    "    print(abstracts[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iony's work work work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, paper_:Counter):\n",
    "    return paper_.get(term)/paper_.total()\n",
    "\n",
    "def inv_doc_freq(term_, cluster_:dict):\n",
    "    N = len(cluster_)\n",
    "    docs =0\n",
    "    for _, p_counter in cluster_.items():\n",
    "        if term_ in p_counter:\n",
    "            docs +=1\n",
    "    docs_w_term = tuple((p_id,p_counter) for  p_id, p_counter in cluster_.items() if term_ in p_counter.keys())\n",
    "    return np.log(N/len(docs_w_term))\n",
    "\n",
    "tf_idf = lambda term, paper, cluster: term_freq(term, paper) * inv_doc_freq(term, cluster)\n",
    "\n",
    "def paper_tfidf(paper_:Counter, cluster_: dict):\n",
    "    terms_tfidf = Counter()\n",
    "    for term in paper_:\n",
    "        terms_tfidf[term] = tf_idf(term, paper_, cluster_)\n",
    "    return terms_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01f02fae-97df-4207-a386-a1bc8ec0853b\n",
      "076bca9b-b30c-4c2c-8148-3b9b5d9aa939\n"
     ]
    }
   ],
   "source": [
    "cluster0_keys = TF_papers[0].keys()\n",
    "\n",
    "print(list(cluster0_keys)[0])\n",
    "print(list(cluster0_keys)[1])\n",
    "print(list(cluster0_keys)[2])\n",
    "\n",
    "cluster_dummy = {\n",
    "    '01f02fae-97df-4207-a386-a1bc8ec0853b':TF_papers[0]['01f02fae-97df-4207-a386-a1bc8ec0853b'],\n",
    "    '076bca9b-b30c-4c2c-8148-3b9b5d9aa939':TF_papers[0]['076bca9b-b30c-4c2c-8148-3b9b5d9aa939']\n",
    "}\n",
    "\n",
    "cluster = TF_papers[0]\n",
    "TFIDFs = dict.fromkeys(cluster.keys())\n",
    "for p_id, p_counter in cluster.items():\n",
    "    TFIDFs[p_id] = paper_tfidf(p_counter, cluster)\n",
    "    \n",
    "c0_df = DataFrame(columns=list(TF_clusters[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_papers(word,papers):\n",
    "    occ = sum(1 for paper in papers.values() if word in paper)\n",
    "    return np.log(10 / occ)\n",
    "\n",
    "def tf_idf_papers(cluster, paper_id, counter): \n",
    "    print(paper_id)\n",
    "    total_words = len(counter)\n",
    "    return {word: (counter[word] / total_words) * idf_papers(word, TF_papers[cluster]) for word in counter}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
