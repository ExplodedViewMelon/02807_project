{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from dataloader import *\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sub_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# compute the best partition\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# compute modularity\n",
    "mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "number_of_communities = len(set(partition.values()))\n",
    "print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "#pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"partition.pkl\", 'rb') as f:\n",
    "    partition = pickle.load(f)\n",
    "    \n",
    "with open(\"mod.pkl\", 'rb') as f:\n",
    "    mod = pickle.load(f)  \n",
    "    \n",
    "number_of_communities = len(set(partition.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "hist = plt.hist(community_size, bins=100, range=(3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = nx.get_node_attributes(G, 'abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#community_text = {i: ' '.join(abstracts[k] for j in community_dict[i] for k in j) for i in community_dict}#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(community_text[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "community_text = {}\n",
    "for i in community_dict:\n",
    "    community_text[i] = ''\n",
    "    for j in community_dict[i]:\n",
    "        for k in j:\n",
    "            community_text[i] = community_text[i] + abstracts[k]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(community_text, open( \"community_text.pkl\", \"wb\" ) )\n",
    "with open(\"community_text.pkl\", 'rb') as f:\n",
    "    community_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(community_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)  # Tokenize text\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to each text in community_text\n",
    "community_text_clean = {i: clean_and_tokenize(text) for i, text in community_text.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(community_text_clean, open( \"community_text_clean.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "text_dict = {}\n",
    "\n",
    "for i in community_text:\n",
    "    no_punct = re.sub(r'[^\\w\\s]','',community_text[i])\n",
    "    no_punct = re.sub(r'\\n' , ' ', no_punct)\n",
    "    no_punct = no_punct.lower()\n",
    "    tokens = nltk.word_tokenize(no_punct)\n",
    "    text_dict[i] = nltk.Text(tokens)\n",
    "    \n",
    "pickle.dump(text_dict, open( \"text_dict.pkl\", \"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#text_dict = {i: nltk.Text(nltk.word_tokenize(re.sub(r'[^\\w\\s]|[\\n]', ' ', community_text[i]).lower())) for i in community_text}\n",
    "#pickle.dump(text_dict, open(\"text_dict.pkl\", \"wb\"))\n",
    "\n",
    "with open(\"text_dict.pkl\", 'rb') as f:\n",
    "    text_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "TF = {}\n",
    "word_set = set()\n",
    "\n",
    "for c in community_text:\n",
    "    overall_freq = Counter()\n",
    "    for i in community_text[c]:\n",
    "        try:\n",
    "            fd = nltk.FreqDist(text_dict[i])\n",
    "            word_set.update(set(list(fd.keys())))\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "        except:\n",
    "            continue\n",
    "    TF[c] = overall_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = {}\n",
    "word_set = set()\n",
    "c = 2\n",
    "\n",
    "overall_freq = Counter()\n",
    "for i in community_text_clean[c]:\n",
    "    try:\n",
    "        print(i)\n",
    "        fd = nltk.FreqDist(text_dict[i])\n",
    "        word_set.update(set(list(fd.keys())))\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "    except:\n",
    "        continue\n",
    "TF[c] = overall_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict['objects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(TF, open(\"TF.pkl\", \"wb\"))\n",
    "with open(\"TF.pkl\", 'rb') as f:\n",
    "    TF = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "TF = defaultdict(Counter)\n",
    "word_set = set()\n",
    "\n",
    "for c in community_text:\n",
    "    for i in community_text[c]:\n",
    "        try:\n",
    "            fd = nltk.FreqDist(text_dict[i])\n",
    "            word_set.update(fd.keys())\n",
    "            TF[c] += fd\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    occ = sum(1 for i in TF if word in TF[i])\n",
    "    return np.log(10 / max(1, occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(number, com):    \n",
    "    total_words = len(TF[number])\n",
    "    return {word: (TF[number][word] / total_words) * idf(word) for word in com}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_all_communities = [tf_idf(i, TF.get(i)) for i in TF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_idf_all_communities, open(\"tf_idf_all_communities.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    occ = 0\n",
    "    for i in TF:\n",
    "        com = TF.get(i)\n",
    "        if word in com.keys():\n",
    "            occ += 1\n",
    "    if occ == 0:\n",
    "        occ = 1\n",
    "    return np.log(10/occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(number, com):    \n",
    "    vec = {}\n",
    "    # Go through every word \n",
    "    for word in com.keys():\n",
    "\n",
    "        # calculate the term frequency by dividing\n",
    "        # the occurance of a certain word with the\n",
    "        # total number of words in the commnity\n",
    "        tf = TF[number][word]/len(TF[number])\n",
    "        \n",
    "        # calculates the inverse document frequency,\n",
    "        idfreq = idf(word)\n",
    "\n",
    "        # Then we multiply the two measures \n",
    "        val = tf*idfreq\n",
    "\n",
    "        # save the result in a dictionary with the word\n",
    "        # as the key and the TF-IDF score as the value\n",
    "        vec[word] = val\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_all_communities = []\n",
    "for i in TF:\n",
    "    v = tf_idf(i, TF.get(i))\n",
    "    TF_IDF_all_communities.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_all_communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = defaultdict(Counter)\n",
    "word_set = set()\n",
    "\n",
    "for i in community_text:\n",
    "    # Join all sentences into a single string, then split and count\n",
    "    overall_freq = Counter(' '.join(community_text[i]).split())\n",
    "    TF[i] = overall_freq\n",
    "    word_set |= set(TF[i].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = Counter()\n",
    "for word in word_set:\n",
    "    for i in community_text:\n",
    "        if word in TF[i]:\n",
    "            DF[word] += 1\n",
    "\n",
    "# Calculate Inverse Document Frequency (IDF)\n",
    "N = len(community_text)\n",
    "IDF = {word: math.log(N / DF[word]) for word in DF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF\n",
    "TF_IDF = {i: {word: TF[i][word] * IDF[word] for word in TF[i]} for i in community_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
