{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach was initially implemented to work as a commandline too where the prompt would be passed as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import get_referenced_by, filter_df, clean_text\n",
    "from dataloader import CitationDataset\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text: str, shingle_size):\n",
    "        text_list = text.split()\n",
    "        return list(set(\" \".join(text_list[i:i+shingle_size]) for i in range(len(text_list)-shingle_size+1)))\n",
    "    \n",
    "def minhash(text_list, seed) -> int:\n",
    "    hash_list = [mmh3.hash(shingle, seed) for shingle in text_list] \n",
    "    return min(hash_list)\n",
    "\n",
    "def get_signature(text: str, shingle_size = 3, sig_len = 50):    \n",
    "    shingle_list = shingle(text, shingle_size)\n",
    "    if len(shingle_list) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        signature = [minhash(shingle_list, seed) for seed in range(sig_len)]\n",
    "    except  Exception as e:\n",
    "        print(text)\n",
    "        print(shingle_list)\n",
    "        sys.exit(e)\n",
    "    return signature\n",
    "\n",
    "\n",
    "def get_model(save_path: str, shingle_size=2, signature_size=250, subset=False) -> pd.DataFrame:\n",
    "    \"\"\" Loads model from cache or creates it from scratch using the given parameters\n",
    "    based on the dataset from the dataloader. \n",
    "\n",
    "    Args:\n",
    "        save_path (str): Path to cache file\n",
    "        shingle_size (int, optional): Size to shingle the abstracts to. Defaults to 2.\n",
    "        signature_size (int, optional): length of the signature. Defaults to 250.\n",
    "        subset (bool, optional): Whether to use a smaller model for demonstration. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the signatures and other information\n",
    "    \"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"Loading cached dataframe...\")\n",
    "        with open(save_path, \"rb\") as f:\n",
    "            df = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Loading dataframe...\")\n",
    "        dataset = CitationDataset()\n",
    "        df = dataset.load_dataframe(subset=subset)\n",
    "        \n",
    "        # get and count the internal references\n",
    "        df = get_referenced_by(df)\n",
    "        print(\"Filtering dataframe...\")\n",
    "        df = filter_df(df)\n",
    "\n",
    "        print(\"Calculating signatures...\")\n",
    "        df[\"signature\"] = df[\"abstract\"].parallel_apply(get_signature, \n",
    "                                                        shingle_size=shingle_size, \n",
    "                                                        sig_len=signature_size)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(df, f)\n",
    "    return df\n",
    "\n",
    "\n",
    "def jaccard(signature1, signature2):\n",
    "    import numpy as np\n",
    "\n",
    "    if signature1 == np.nan or signature2 == np.nan:\n",
    "        return 0\n",
    "\n",
    "    signatures_doc1 = np.array(signature1)\n",
    "    signatures_doc2 = np.array(signature2)\n",
    "    return len(np.intersect1d(signatures_doc1, signatures_doc2)) / len(\n",
    "        np.union1d(signatures_doc1, signatures_doc2)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_most_similar(df, promt, shingle_size=2, signature_size=250, n_top=10):\n",
    "    clean_promt = clean_text(promt)\n",
    "    promt_sig = get_signature(clean_promt, \n",
    "                              shingle_size=shingle_size, \n",
    "                              sig_len=signature_size)\n",
    "\n",
    "    df[\"sim\"] = df[\"signature\"].parallel_apply(jaccard, signature2=promt_sig)\n",
    "    \n",
    "    return df.nlargest(n_top, [\"sim\", \"n_counted_citations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Loading dataframe...\n",
      "loading dataframe from cache /mnt/c/Users/uroko/OneDrive/DTU/tools_for_data_science/02807_project/DATA/dblp-ref\n",
      "loading /mnt/c/Users/uroko/OneDrive/DTU/tools_for_data_science/02807_project/DATA/dblp-ref/dblp-ref-3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reversing references: 100%|██████████| 79007/79007 [00:04<00:00, 17283.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataframe...\n",
      "Calculating signatures...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5036588bb1d04d70b4d979d79fba4b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3377), Label(value='0 / 3377'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prompt='machine translation for cats'\n",
      "Finding most similar papers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8affecdbf945979833b9634b2ff43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3377), Label(value='0 / 3377'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to test.search_result.json...\n"
     ]
    }
   ],
   "source": [
    "# setup parameters for the model\n",
    "shingle_size = 2\n",
    "signature_length = 250\n",
    "subset = True   # whether to use a subset of the dataset\n",
    "prompt = None\n",
    "threads = 10\n",
    "prompt = \"machine translation for cats\"     # Input prompt here\n",
    "output_file = \"test.search_result.json\"\n",
    "    \n",
    "pandarallel.initialize(nb_workers=threads, progress_bar=True)\n",
    "\n",
    "model_name = f\"model_df_{shingle_size}_{signature_length}{'_subset' if subset else ''}1.pkl\"\n",
    "model_df = get_model(model_name, \n",
    "                    shingle_size=shingle_size, \n",
    "                    signature_size=signature_length, \n",
    "                    subset=subset)\n",
    "\n",
    "if not prompt:\n",
    "    print(\"No prompt given, using random abstract from dataset.\")\n",
    "    test_idx = 500\n",
    "    prompt = model_df.iloc[test_idx][\"abstract\"]\n",
    "    model_df = model_df.drop([test_idx], axis=0).reset_index(drop=True)\n",
    "\n",
    "print()\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "print(\"Finding most similar papers...\")\n",
    "df_top = get_most_similar(model_df, \n",
    "                          prompt,\n",
    "                          shingle_size=shingle_size,\n",
    "                          signature_size=signature_length, \n",
    "                          n_top=10)\n",
    "print()\n",
    "\n",
    "print(f\"Writing results to {output_file}...\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(json.dumps(json.loads(df_top[['title', 'abstract', 'authors', 'sim', 'n_references', 'n_counted_citations']].to_json(orient='records')), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import itertools\n",
    "import pickle \n",
    "import networkx as nx\n",
    "from collections import deque, defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if you're starting over\n",
    "new_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_graph.pkl\", 'rb') as f:\n",
    "    G_directed = pickle.load(f)\n",
    "f.close()\n",
    "G = G_directed.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "if new_df:\n",
    "    # compute the best partition\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # compute modularity\n",
    "    mod = community_louvain.modularity(partition, G)\n",
    "\n",
    "    number_of_communities = len(set(partition.values()))\n",
    "    print('Using the Louvain algortihm we identified', number_of_communities, 'communities')\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(partition, open( \"partition.pkl\", \"wb\" ) )\n",
    "    pickle.dump(mod, open( \"mod.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"partition.pkl\", 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    with open(\"mod.pkl\", 'rb') as f:\n",
    "        mod = pickle.load(f)  \n",
    "        \n",
    "    number_of_communities = len(set(partition.values()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_paths(G, root):\n",
    "    shortest_paths_dict = {root: [[root]]}\n",
    "    queue = deque([(root, [root])])\n",
    "\n",
    "    while queue:\n",
    "        s, path = queue.popleft()\n",
    "\n",
    "        for neighbor in G.neighbors(s):\n",
    "            new_path = path + [neighbor]\n",
    "            old_path = shortest_paths_dict.get(neighbor, [[None] * (len(new_path) + 1)])\n",
    "\n",
    "            if len(new_path) == len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor].append(new_path)\n",
    "            elif len(new_path) < len(old_path[0]):\n",
    "                shortest_paths_dict[neighbor] = [new_path]\n",
    "                queue.append((neighbor, new_path))\n",
    "\n",
    "    return shortest_paths_dict\n",
    "\n",
    "def edge_betweenness_centrality(G):\n",
    "    edge_betweenness = defaultdict(float)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        shortest_paths_dict = bfs_shortest_paths(G, node)\n",
    "\n",
    "        for paths in shortest_paths_dict.values():\n",
    "            for path in paths:\n",
    "                for i in range(len(path) - 1):\n",
    "                    edge = (path[i], path[i + 1])\n",
    "                    edge_betweenness[edge] += 1.0\n",
    "\n",
    "    return edge_betweenness\n",
    "\n",
    "def girvan_newman_directed(G):\n",
    "    G_copy = G.copy()\n",
    "    communities = list(nx.weakly_connected_components(G_copy))\n",
    "    results = {0: communities}\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    while G_copy.number_of_edges() > 0:\n",
    "        edge_betweenness = edge_betweenness_centrality(G_copy)\n",
    "        max_betweenness = max(edge_betweenness.values())\n",
    "        highest_betweenness_edges = [edge for edge, value in edge_betweenness.items() if value == max_betweenness]\n",
    "        G_copy.remove_edges_from(highest_betweenness_edges)\n",
    "        components = list(nx.weakly_connected_components(G_copy))\n",
    "        results[step] = components\n",
    "        step += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def modularity(G, clusters_list):\n",
    "    Q = 0\n",
    "    m = len(list(G.edges()))\n",
    "    for aCommunity in clusters_list:\n",
    "        print(\"aCommunity\", aCommunity)\n",
    "        for v in list(aCommunity):\n",
    "            for w in list(aCommunity):\n",
    "                if v != w:\n",
    "                    avw = 1 if (v,w) in list(G.edges()) or (w,v) in list(G.edges()) else 0               \n",
    "                    new_term = avw - (G.degree(v)*G.degree(w))/(2*m)\n",
    "                    Q += new_term\n",
    "    return Q/(2*m)\n",
    "\n",
    "def compute_modularity_for_all_communities(G, all_communities):\n",
    "    result = []\n",
    "    t = tqdm(total=len(list(all_communities.values())))\n",
    "    for aCommunityRepartition in list(all_communities.values()):\n",
    "        t.update()\n",
    "        aModularity = modularity(G, aCommunityRepartition)\n",
    "        result.append(\n",
    "            [aCommunityRepartition, aModularity]\n",
    "        )\n",
    "    t.close    \n",
    "    return result\n",
    "\n",
    "\n",
    "print('Finding communities...')\n",
    "all_com = girvan_newman_directed(G)\n",
    "\n",
    "print('Finding the modularity...')    \n",
    "all_clusters_with_modularity = compute_modularity_for_all_communities(G, all_com)\n",
    "\n",
    "print('Sorting')\n",
    "all_clusters_with_modularity.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print('Finding the best and pickling')\n",
    "best_cluster = all_clusters_with_modularity[0]\n",
    "print(best_cluster)\n",
    "#pickle.dump(best_cluster, open( \"best_cluster.pkl\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dict where the key is the cluster number and the values\n",
    "'''\n",
    "community_dict[2] = [['27c5ea64-86cb-4e69-9d13-c8ba2654515d'],\n",
    " ['2ee9a087-6188-4ebd-95b9-6561cba0584c'],\n",
    " ['efe2dd1d-706c-4ab6-bd9b-90d35a81d04f']]\n",
    "'''\n",
    "\n",
    "community_dict = {new_list: [] for new_list in range(number_of_communities)}\n",
    "for i, j in partition.items():  \n",
    "    community_dict[j].append([i])\n",
    "    \n",
    "# Filter out communities with only one element\n",
    "community_dict_bigger_than_one = {k: v for k, v in community_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is: 9085 communities with only 1 member\n",
      "There is: 1515 communities with more than 1 member\n"
     ]
    }
   ],
   "source": [
    "community_size_bigger_than_one = np.zeros(len(community_dict_bigger_than_one))\n",
    "\n",
    "for i,j in enumerate(community_dict_bigger_than_one):\n",
    "    community_size_bigger_than_one[i] = (len(community_dict_bigger_than_one[j])) \n",
    "\n",
    "community_size = np.zeros(number_of_communities)\n",
    "\n",
    "for i,j in enumerate(community_dict):\n",
    "    community_size[i] = (len(community_dict[j]))\n",
    "    \n",
    "print('There is:', sum(community_size == 1), 'communities with only 1 member')\n",
    "print('There is:', len(community_dict_bigger_than_one),'communities with more than 1 member')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(community_dict_bigger_than_one, bins=40)\n",
    "plt.xlabel('Community members')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Community size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)# Tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **paper** with their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenized words in each paper\n",
    "if new_df:\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': \"For stereoscopic optical see-through head-mounted display...\n",
    "    abstracts = nx.get_node_attributes(G, 'abstract')\n",
    "    \n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': 'Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration'\n",
    "    titles = nx.get_node_attributes(G, 'title')\n",
    "\n",
    "    # {'01f02fae-97df-4207-a386-a1bc8ec0853b': title + abstract}\n",
    "    paper_dict = {key: titles.get(key, '') + ' ' + abstracts.get(key, '') for key in set(abstracts) | set(titles)}\n",
    "    \n",
    "    # paper_dict_clean['345a2369-8198-46db-8ebb-b3f622b35381'] = ['scalable', 'feature', 'extraction', 'for', 'coarse',\n",
    "    paper_dict_clean = {key:  clean_and_tokenize(text) for key, text in paper_dict.items()}\n",
    "    \n",
    "    print('Pickeling...')\n",
    "    pickle.dump(paper_dict_clean, open( \"paper_dict_clean.pkl\", \"wb\" ) )\n",
    "    pickle.dump(abstracts, open( \"abstracts.pkl\", \"wb\" ) )\n",
    "    pickle.dump(titles, open( \"titles.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open(\"paper_dict_clean.pkl\", 'rb') as f:\n",
    "        paper_dict_clean = pickle.load(f)\n",
    "    \n",
    "    with open(\"abstracts.pkl\", 'rb') as f:\n",
    "        abstracts = pickle.load(f)\n",
    "    \n",
    "    with open(\"titles.pkl\", 'rb') as f:\n",
    "        titles = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dict for each **cluster** with all words from the papers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if new_df:\n",
    "    community_texts_clean = {new_list: [] for new_list in range(len(community_dict_bigger_than_one))}\n",
    "    \n",
    "    for cluster_id, paper_ids in enumerate(community_dict_bigger_than_one.values()):\n",
    "        for paper_id in paper_ids:  \n",
    "            community_texts_clean[cluster_id].extend(paper_dict_clean[paper_id[0]])          \n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(community_texts_clean, open( \"community_texts_clean.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"community_texts_clean.pkl\", 'rb') as f:\n",
    "        community_texts_clean = pickle.load(f)\n",
    " \n",
    "        \n",
    "# Turn into nltk format\n",
    "community_text_clean_text = { cluster_id: nltk.Text(text) for cluster_id, text in community_texts_clean.items() } \n",
    "\n",
    "# Example \n",
    "# {'345a2369-8198-46db-8ebb-b3f622b35381': <Text: scalable feature extraction for coarse to fine jpeg...>,\n",
    "paper_dict_clean_text = { paper_id: nltk.Text(text) for paper_id, text in paper_dict_clean.items() } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **community**, looking at the 'document' as a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for each community\n",
    "if new_df:\n",
    "    \n",
    "    # TF_clusters[100] = Counter({'the': 28, 'of': 23,'in': 14,'we': 13,\n",
    "    TF_clusters = {}\n",
    "\n",
    "    for cluster_, text in enumerate(community_text_clean_text.values()):\n",
    "        overall_freq = Counter()           \n",
    "        try:\n",
    "            fd = nltk.FreqDist(text)\n",
    "            overall_freq = overall_freq + Counter(fd)\n",
    "        except:\n",
    "            print('Breaked')\n",
    "            continue\n",
    "            \n",
    "        TF_clusters[cluster_] = overall_freq\n",
    "\n",
    "    pickle.dump(TF_clusters, open( \"TF_clusters.pkl\", \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    with open(\"TF_clusters.pkl\", 'rb') as f:\n",
    "        TF_clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF for each **paper** inside their community, looking at the 'document' as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_df:    \n",
    "    TF_papers = {}\n",
    "\n",
    "    for cluster_, paper_ids in enumerate(community_dict_bigger_than_one.values()): \n",
    "        TF_papers[cluster_] = {}\n",
    "        \n",
    "        for paper_id in paper_ids:         \n",
    "            overall_freq = Counter()     \n",
    "                    \n",
    "            try:\n",
    "                text = paper_dict_clean_text[paper_id[0]]\n",
    "                fd = nltk.FreqDist(text)\n",
    "                overall_freq = overall_freq + Counter(fd)\n",
    "                \n",
    "            except:\n",
    "                print('Breaked')\n",
    "                continue\n",
    "            \n",
    "            TF_papers[cluster_][paper_id[0]] = overall_freq\n",
    "            \n",
    "    pickle.dump(TF_papers, open( \"TF_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"TF_papers.pkl\", 'rb') as f:\n",
    "        TF_papers = pickle.load(f)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF for all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_items(dict_, n):\n",
    "    # Sort the dictionary by value in descending order and get the top n items\n",
    "    top_n_items = sorted(dict_.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Convert the list of tuples back to a dictionary\n",
    "    return dict(top_n_items)\n",
    "\n",
    "def tf_idf(cluster_id, counter):  \n",
    "    # Total number of words in the cluster\n",
    "    total_words = counter.total()\n",
    "    \n",
    "    return {word: (TF_clusters[cluster_id][word] / total_words) * np.log( N / word_count(word) ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "\n",
    "    # Number of clusters\n",
    "    N = len(TF_clusters)\n",
    "    \n",
    "    tf_idf_all_communities = {i: tf_idf(i, TF_clusters[i]) for i in TF_clusters}\n",
    "    top_tf_idf_all_communities = {i: get_top_n_items(tf_idf_all_communities[i], 40) for i in tf_idf_all_communities}\n",
    "    \n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_communities, open( \"tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    pickle.dump(top_tf_idf_all_communities, open( \"top_tf_idf_all_communities.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        tf_idf_all_communities = pickle.load(f)\n",
    "    with open(\"top_tf_idf_all_communities.pkl\", 'rb') as f:\n",
    "        top_tf_idf_all_communities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1515/1515 [03:14<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_items(dict_, n):\n",
    "    # Sort the dictionary by value in descending order and get the top n items\n",
    "    top_n_items = sorted(dict_.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Convert the list of tuples back to a dictionary\n",
    "    return dict(top_n_items)\n",
    "\n",
    "def tf_idf_papers(counter): \n",
    "    # Total words in the current paper\n",
    "    total_words = counter.total()\n",
    "\n",
    "    return {word: (counter[word] / total_words) * np.log( N / word_count_papers[cluster_id][word] ) for word in counter}\n",
    "\n",
    "if new_df:\n",
    "    # Dict to store the word counts\n",
    "    word_count_papers = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i in TF_papers:\n",
    "        for paper in TF_papers[i].values():\n",
    "            for word in paper:\n",
    "                word_count_papers[i][word] += 1\n",
    "\n",
    "    tf_idf_all_papers = {}\n",
    "    top_tf_idf_all_papers = {}\n",
    "\n",
    "    for cluster_id, papers in tqdm(TF_papers.items(), desc=\"Processing clusters\"):\n",
    "        # Number of papers inside the current cluster \n",
    "        N = len(TF_papers[cluster_id])\n",
    "\n",
    "        tf_idf_all_papers[cluster_id] = {paper_id_: tf_idf_papers(counter_) for paper_id_, counter_ in papers.items()}\n",
    "        top_tf_idf_all_papers[cluster_id] = {paper_id_: get_top_n_items(tf_idf_all_papers[cluster_id][paper_id_], 40) for paper_id_ in tf_idf_all_papers[cluster_id]}\n",
    "\n",
    "    print('Pickling...')\n",
    "    pickle.dump(tf_idf_all_papers, open( \"tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    pickle.dump(top_tf_idf_all_papers, open( \"top_tf_idf_all_papers.pkl\", \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    with open(\"tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        tf_idf_all_papers = pickle.load(f)\n",
    "    with open(\"top_tf_idf_all_papers.pkl\", 'rb') as f:\n",
    "        top_tf_idf_all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF for user-input. At cluster level\n",
    "def text_cluster(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf(word):\n",
    "    return max(1, sum(word in cluster for cluster in TF_clusters.values()))\n",
    "\n",
    "def text_tf_idf(tf_words):\n",
    "    tf_idf_scores = {}\n",
    "    N = len(TF_clusters) \n",
    "    for word in tf_words:\n",
    "        tf = TF_text[word]/TF_text.total()\n",
    "        idfreq = np.log( N / text_idf(word) )\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores\n",
    "\n",
    "### Jaccard similarity\n",
    "def jaccard_similarity(tfidf, keywords):\n",
    "    keywords_40 = [x[0] for x in keywords]\n",
    "    set_input_words = set(tfidf.keys())\n",
    "    set_keywords = set(keywords_40)\n",
    "    \n",
    "    intersection = len(set_input_words & set_keywords)\n",
    "    union = len(set_input_words | set_keywords)\n",
    "    \n",
    "    similarity = intersection / union if union != 0 else 0\n",
    "    return similarity\n",
    "\n",
    "### TF-IDF inside the cluster\n",
    "def text_paper(text):\n",
    "    overall_freq = Counter() \n",
    "    text = nltk.Text(clean_and_tokenize(text))\n",
    "           \n",
    "    try:\n",
    "        fd = nltk.FreqDist(text)\n",
    "        overall_freq = overall_freq + Counter(fd)\n",
    "        \n",
    "    except:\n",
    "        print('Breaked')\n",
    "            \n",
    "    return overall_freq\n",
    "\n",
    "def text_idf_paper(word):\n",
    "    return max(1,sum(word in cluster for cluster in TF_papers[best_cluster].values()))\n",
    "\n",
    "def text_tf_idf_paper(tf_words, best_cluster):\n",
    "    N = len(TF_papers[best_cluster])\n",
    "    tf_idf_scores = {} \n",
    "    for word in tf_words:\n",
    "        tf = TF_paper[word]/TF_paper.total()\n",
    "        idfreq = np.log(N / text_idf_paper(word))\n",
    "        tf_idf_scores[word] = tf*idfreq\n",
    "            \n",
    "    return tf_idf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual user input box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input:\n",
      " A Heterogeneous System for Real-Time Detection with AdaBoost. \n",
      "\n",
      "12 highest TF-IDF scores:\n",
      "[('adaboost', 0.4865257487530032), ('heterogeneous', 0.3790164125017026), ('detection', 0.2987157477459815), ('real', 0.20565556047466607), ('time', 0.12678731235853752), ('system', 0.11690914289835223), ('with', 0.017863691176416865), ('for', 0.008771466926679941), ('a', 0.0020727523870349334)]\n",
      "\n",
      "## Best matching clusters: ##\n",
      "275 388 860\n",
      "\n",
      "### Cluster 275 - Papers with highest jaccard similarity ###\n",
      "( Paper id, jaccard similarity score) \n",
      "('b8a1b824-0f91-4dbe-a4bf-ee36e1eece95', 0.06521739130434782)\n",
      "('918c35a9-e938-4273-bec4-5b8ec5252daa', 0.0425531914893617)\n",
      "('20b9ea3f-23cb-4c55-a3c4-b3b94df602df', 0.0)\n",
      "\n",
      "### Cluster 388 - Papers with highest jaccard similarity ###\n",
      "( Paper id, jaccard similarity score) \n",
      "('f8605647-8e39-4ffd-b90d-1cddab2cf87a', 0.06666666666666667)\n",
      "('bb8f2ac2-1150-405e-b4eb-a1e1e8042b45', 0.06451612903225806)\n",
      "\n",
      "### Cluster 860 - Papers with highest jaccard similarity ###\n",
      "( Paper id, jaccard similarity score) \n",
      "('6f928e33-7239-458d-a458-6613a40d22f0', 0.020833333333333332)\n",
      "('28229e6a-474d-4a38-8b0c-a027039bbc6d', 0.0)\n",
      "('c8cd41f2-216d-450b-9e03-0548cf485537', 0.0)\n",
      "\n",
      "### Papers recommended for cluster 275 ###\n",
      "Title:  An Intranet solution for a real-time GPMS in newspaper production\n",
      "Abstract:  Newspaper production is a complex and often suboptimised process, characterised by heterogeneous computer based production tools in a distributed environment. Attempts to optimize this process have generally been on a local subsystem level, without a global perspective. With the emerging IFRAtrack standard for exchanging local and global tracking information, new possibilities for global optimization are emerging. This paper presents the Intranet client-server solution we have chosen as the platform for a global production management system (GPMS) aimed at solving these problems. A production database contains the relevant information supplied by the local systems and acts as a global collector of production information. A web server communicates with the database through scripts and other mechanisms to dynamically retrieve and update its contents. Users interact with the system using WWW browsers, which provide the basis for the client user interface. Distributed code modules, applets, can provide functionality not supported by HTML. \n",
      "\n",
      "Title:  Using a simulator for testing and validating a newspaper production decision support system\n",
      "Abstract:  The paper presents and discusses the use of a software simulation tool for testing and validating a production decision support system. The developed simulator is specially designed for simulation of the entire newspaper production process. The authors propose methods for testing and validating a global production management system (GPMS), before implementing it in the production environment. Newspaper production is a complex time-critical process characterized by heterogeneous computer based production tools in a distributed environment. A GPMS is an application of decision support systems for the entire newspaper production process, and needs to collect events from various production subsystems to get the current production state. In order to test a GPMS without disturbing the daily production, there is a need for a simulation tool which must simulate the event providers. This can only be achieved by using a strictly specified communication mechanism between the event providers and the GPMS. \n",
      "\n",
      "Title:  A simulation of the product distribution in the newspaper industry\n",
      "Abstract:  On-time delivery of the paper is critical in the newspaper industry since it is directly related to the quality of service; therefore, it enhances or damages sales. This study was focused on determining the means to meet required delivery times, and in identifying ways to improve it, so that there is a lower probability of failure. A simulation model was built to model the paper printing, packaging, and distribution processes. The results yielded a press schedule and warehouse assignments that provided a 13% improvement on delivery time. This improvement results in a 97% on time delivery of the paper. \n",
      "\n",
      "\n",
      "### Papers recommended for cluster 388 ###\n",
      "Title:  Managing the programming tail\n",
      "Abstract:  We describe a way of dealing with those students in a class learning programming that are unable to keep up with the rest. \n",
      "\n",
      "Title:  A learning object generator for programming\n",
      "Abstract:  We introduce a general tool for creating small learning scenarios (Learning Objects) that can be used by teachers and students for teaching/learning Java. \n",
      "\n",
      "\n",
      "### Papers recommended for cluster 860 ###\n",
      "Title:  A combined zone-3 relay blocking and sensitivity-based load shedding for voltage collapse prevention\n",
      "Abstract:  A typical power system voltage collapse scenarios is often ended with the undesirable operation of the Zone-3 distance relay of the transmission lines. This paper presents a protection scheme to avoid power system voltage collapse using a combined method of distance relay's Zone-3 blocking scheme and a sensitivity-based load shedding selection. The Zone-3 distance relay blocking is based on the proper differentiation between transmission line overloading and line faulted conditions, using a fast estimation of power flow based on Line Outage Distribution Factor (LODF) and Generation Shift Factor (GSF). The Zone-3 distance relay of the transmission line would be blocked if the power flow change over the line is determined to be due to an overload so that more time would be available for the system to take necessary control actions. One of the important control actions is the emergency load shedding. A method based on the calculated sensitivities GSF to identify the most effective load shedding positions and amounts is proposed. The proposed method has been implemented in the Advanced Real-Time Interactive Simulator for Training and Operation (ARISTO) software with the Nordic 32-bus test system. ARISTO offers the possibility to test the proposed scheme since it can be seen as the virtual power system with all live information. The analyses of power system voltage collapse scenarios with and without the proposed scheme implemented have shown the effectiveness of the scheme to prevent the voltage collapses. \n",
      "\n",
      "Title:  Protection system monitoring for the prevention of cascading events in smart transmission grids\n",
      "Abstract:  With the diffusion of standardized communication protocols in smart transmission systems, it is expected that digital distance relays will become active elements in monitoring and control architecture. Real-time tuning of distance relays settings will allow to overcome the classical conflict between dependability and security, or to avoid improper operation of such devices in vulnerable conditions and during major blackout events. In this paper, along with the presentation of a monitoring and control architecture that integrates such devices into SCADA/EMS, a system security monitoring function, based on simulated system trajectories and their closeness to distance relay zones, is proposed for system operation and contingency analysis. \n",
      "\n",
      "Title:  Controlling transient stability through line switching\n",
      "Abstract:  Smart transmission systems have introduced new possibilities of using topology control process for increasing power system dynamic security. Nowadays, in short term analysis, transmission topology is often treated as static and network reconfiguration is performed only for optimizing power flows and market performance. Modern technological infrastructure have enabled fast communication between power system elements. In this scenario, it is foreseeable that topology control can be exploited in real-time as the last resource for avoiding system collapse. In this paper, a dynamic corrective control strategy based on line switching technique is presented. In particular, the proposed method is based on an algorithm that allows to identify transmission lines that should be switched on/off during transients and the time when the corrective action has to be applied in order to guarantee system stability. The feasibility of this approach is demonstrated through simulations on a detailed representation of the Italian transmission network. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "print('The user input:\\n', text)\n",
    "\n",
    "# TF for the input text\n",
    "TF_text = text_cluster(text)\n",
    "\n",
    "# TF-IDF for the input text on cluster level\n",
    "cLevel_tf_idf = text_tf_idf(TF_text)\n",
    "cLevel_sorted = sorted(cLevel_tf_idf.items(), key=lambda x:x[1], reverse=True)[:12]\n",
    "print('\\n12 highest TF-IDF scores:')\n",
    "print(cLevel_sorted)\n",
    "\n",
    "# Similarity on cluster-level\n",
    "sim_cluster = {}\n",
    "for i in range(len(tf_idf_all_communities)):\n",
    "    sim_cluster[i] = jaccard_similarity(top_tf_idf_all_communities[i], cLevel_sorted)\n",
    "   \n",
    "# Find the most similar clusters \n",
    "sorted_similarity = sorted(sim_cluster.items(), key=lambda x:x[1], reverse=True)[:5]\n",
    "    \n",
    "c1, c2, c3 = sorted_similarity[0][0], sorted_similarity[1][0], sorted_similarity[2][0]\n",
    "clusters = [c1,c2,c3]\n",
    "\n",
    "print('\\n## Best matching clusters: ##')\n",
    "print(c1, c2, c3)\n",
    "\n",
    "########## PAPER LEVEL ##########\n",
    "# TF on input text inside the choosen clusters\n",
    "TF_paper = text_paper(text)\n",
    "\n",
    "# TF-IDF words \n",
    "dLevel_sorted = {}\n",
    "for i in clusters:\n",
    "    c = text_tf_idf_paper(TF_paper, i)\n",
    "    dLevel_sorted[i] = sorted(c.items(), key=lambda x:x[1], reverse=True)[:40]\n",
    "\n",
    "#{585: {'73d121f9-0014-434c-b689-e1024ebb95f8': {0.03896103896103896}, 'f188c8be-987b-4fce-984a-95a5105e5a8e': {0.012658227848101266}},\n",
    "sim_papers = {}\n",
    "for cluster in clusters:\n",
    "    sim_papers[cluster] = {}\n",
    "    dLevel_words = dLevel_sorted[cluster]\n",
    "    for n in TF_papers[cluster]:\n",
    "        sim_papers[cluster][n] = jaccard_similarity(top_tf_idf_all_papers[cluster][n], dLevel_words)\n",
    "\n",
    "\n",
    "sorted_similarity_papers = {}\n",
    "for cluster in sim_papers:\n",
    "    sorted_similarity_papers[cluster] = sorted(sim_papers[cluster].items(), key=lambda x:x[1], reverse=True)[:3]\n",
    "\n",
    "\n",
    "for cluster in clusters:\n",
    "    print('\\n### Cluster', cluster, '- Papers with highest jaccard similarity ###')\n",
    "    print('( Paper id, jaccard similarity score) ')\n",
    "    for paper in sorted_similarity_papers[cluster]:\n",
    "        print(paper)\n",
    "\n",
    "for cluster in clusters:\n",
    "    print('\\n### Papers recommended for cluster', cluster, '###')\n",
    "    for b in sorted_similarity_papers[cluster]:\n",
    "        print('Title: ',titles[b[0]])\n",
    "        print('Abstract: ', abstracts[b[0]], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to make embeddings, build index and search papers.\n",
    "Making the embeddings of the dataset takes quite a long time to run, ~3 hours on the HPC on GPU.\n",
    "It is possible to run it on a subset of the data by setting use_subset=True which will greatly reduce computation time.\n",
    "Embeddings and index are cached automatically but if they are not yet built, set make_embeddings and make_index to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"./citation_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Search, benchmark\n",
    "\n",
    "# remove use_subset to use the whole citation dataset.\n",
    "S = Search(path_data, make_embeddings=True, make_index=True, use_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top_k nearest papers from search prompt.\n",
    "search_prompt = \"Test search prompt one two three\"\n",
    "S.search_index_vocal(search_prompt, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but return distances and indices\n",
    "distances, neighbor_indices = S.search_index([search_prompt], k=5)\n",
    "print(neighbor_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search by paper id instead of prompt\n",
    "S.find_similar_papers(id=\"001eef4f-1d00-4ae6-8b4f-7e66344bbc6e\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs search for 100 prompts and prints elapsed time.\n",
    "benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
